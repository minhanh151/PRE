***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/ResidualPrompting/vit_b16_ep50_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '8.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '1', 'DATASET.SUBSAMPLE_CLASSES', 'all']
output_dir: output/base2new/train_base/caltech101/shots_1_8.0/ResidualPrompting/vit_b16_ep50_ctxv1/seed1
resume: 
root: /home/ducan/Downloads/MinhAnh/CoOp/DATA
seed: 1
source_domains: None
target_domains: None
trainer: ResidualPrompting
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 1
  ROOT: /home/ducan/Downloads/MinhAnh/CoOp/DATA
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/base2new/train_base/caltech101/shots_1_8.0/ResidualPrompting/vit_b16_ep50_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: amp
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: amp
    W: 8.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: ResidualPrompting
  PLOT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N: 4
    N_CTX: 16
    PREC: amp
  ResidualPrompting:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: amp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.10.2
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.16 (default, Jun 12 2023, 18:09:05)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.17
Is CUDA available: False
CUDA runtime version: 12.2.91
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.2
[pip3] torch==1.10.2
[pip3] torchvision==0.11.3
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h6d00ec8_46342  
[conda] mkl-service               2.4.0            py38h5eee18b_1  
[conda] mkl_fft                   1.3.6            py38h417a72b_1  
[conda] mkl_random                1.2.2            py38h417a72b_1  
[conda] numpy                     1.24.3           py38hf6e8229_1  
[conda] numpy-base                1.24.3           py38h060ed82_1  
[conda] pytorch                   1.10.2          py3.8_cuda10.2_cudnn7.6.5_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.11.3               py38_cu102    pytorch
        Pillow (9.4.0)

Loading trainer: ResidualPrompting
Loading dataset: Caltech101
Reading split from /home/ducan/Downloads/MinhAnh/CoOp/DATA/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /home/ducan/Downloads/MinhAnh/CoOp/DATA/caltech-101/split_fewshot/shot_1-seed_1.pkl
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  100
# train_x  100
# val      100
# test     2,465
---------  ----------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Using skip connection in MLP
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/base2new/train_base/caltech101/shots_1_8.0/ResidualPrompting/vit_b16_ep50_ctxv1/seed1/tensorboard)
epoch [1/50] batch [1/3] time 20.310 (20.310) data 0.406 (0.406) loss 2.3018 (2.3018) acc 50.0000 (50.0000) lr 1.0000e-05 eta 0:50:26
epoch [1/50] batch [2/3] time 18.637 (19.474) data 0.005 (0.205) loss 2.0498 (2.1758) acc 59.3750 (54.6875) lr 1.0000e-05 eta 0:48:02
epoch [1/50] batch [3/3] time 18.641 (19.196) data 0.008 (0.139) loss 2.1903 (2.1807) acc 43.7500 (51.0417) lr 2.0000e-03 eta 0:47:01
epoch [2/50] batch [1/3] time 21.576 (21.576) data 0.580 (0.580) loss 2.1444 (2.1444) acc 50.0000 (50.0000) lr 2.0000e-03 eta 0:52:30
epoch [2/50] batch [2/3] time 18.504 (20.040) data 0.005 (0.292) loss 1.7471 (1.9457) acc 56.2500 (53.1250) lr 2.0000e-03 eta 0:48:25
epoch [2/50] batch [3/3] time 18.268 (19.449) data 0.009 (0.198) loss 1.0860 (1.6592) acc 68.7500 (58.3333) lr 1.9980e-03 eta 0:46:40
epoch [3/50] batch [1/3] time 21.304 (21.304) data 0.574 (0.574) loss 0.8870 (0.8870) acc 81.2500 (81.2500) lr 1.9980e-03 eta 0:50:46
epoch [3/50] batch [2/3] time 18.401 (19.853) data 0.008 (0.291) loss 1.1604 (1.0237) acc 65.6250 (73.4375) lr 1.9980e-03 eta 0:46:59
epoch [3/50] batch [3/3] time 18.896 (19.534) data 0.007 (0.196) loss 0.4500 (0.8325) acc 84.3750 (77.0833) lr 1.9921e-03 eta 0:45:54
epoch [4/50] batch [1/3] time 21.273 (21.273) data 0.632 (0.632) loss 1.0329 (1.0329) acc 71.8750 (71.8750) lr 1.9921e-03 eta 0:49:38
epoch [4/50] batch [2/3] time 18.353 (19.813) data 0.007 (0.320) loss 0.7868 (0.9099) acc 75.0000 (73.4375) lr 1.9921e-03 eta 0:45:53
epoch [4/50] batch [3/3] time 18.305 (19.310) data 0.008 (0.216) loss 0.9611 (0.9269) acc 75.0000 (73.9583) lr 1.9823e-03 eta 0:44:24
epoch [5/50] batch [1/3] time 21.118 (21.118) data 0.676 (0.676) loss 0.7677 (0.7677) acc 75.0000 (75.0000) lr 1.9823e-03 eta 0:48:13
epoch [5/50] batch [2/3] time 18.378 (19.748) data 0.008 (0.342) loss 0.8644 (0.8161) acc 62.5000 (68.7500) lr 1.9823e-03 eta 0:44:45
epoch [5/50] batch [3/3] time 18.224 (19.240) data 0.005 (0.230) loss 0.9993 (0.8771) acc 65.6250 (67.7083) lr 1.9686e-03 eta 0:43:17
epoch [6/50] batch [1/3] time 21.468 (21.468) data 0.683 (0.683) loss 0.4895 (0.4895) acc 90.6250 (90.6250) lr 1.9686e-03 eta 0:47:56
epoch [6/50] batch [2/3] time 18.271 (19.869) data 0.005 (0.344) loss 0.7850 (0.6373) acc 78.1250 (84.3750) lr 1.9686e-03 eta 0:44:02
epoch [6/50] batch [3/3] time 18.642 (19.460) data 0.006 (0.232) loss 0.7964 (0.6903) acc 84.3750 (84.3750) lr 1.9511e-03 eta 0:42:48
epoch [7/50] batch [1/3] time 21.486 (21.486) data 0.761 (0.761) loss 1.5465 (1.5465) acc 65.6250 (65.6250) lr 1.9511e-03 eta 0:46:54
epoch [7/50] batch [2/3] time 18.756 (20.121) data 0.004 (0.382) loss 0.5579 (1.0522) acc 90.6250 (78.1250) lr 1.9511e-03 eta 0:43:35
epoch [7/50] batch [3/3] time 18.677 (19.640) data 0.005 (0.257) loss 0.7533 (0.9526) acc 75.0000 (77.0833) lr 1.9298e-03 eta 0:42:13
epoch [8/50] batch [1/3] time 20.886 (20.886) data 0.571 (0.571) loss 0.6467 (0.6467) acc 87.5000 (87.5000) lr 1.9298e-03 eta 0:44:33
epoch [8/50] batch [2/3] time 18.671 (19.779) data 0.008 (0.290) loss 0.6949 (0.6708) acc 84.3750 (85.9375) lr 1.9298e-03 eta 0:41:51
epoch [8/50] batch [3/3] time 18.583 (19.380) data 0.005 (0.195) loss 0.6902 (0.6773) acc 78.1250 (83.3333) lr 1.9048e-03 eta 0:40:41
epoch [9/50] batch [1/3] time 21.117 (21.117) data 0.591 (0.591) loss 0.8522 (0.8522) acc 84.3750 (84.3750) lr 1.9048e-03 eta 0:43:59
epoch [9/50] batch [2/3] time 18.408 (19.763) data 0.005 (0.298) loss 0.9481 (0.9002) acc 78.1250 (81.2500) lr 1.9048e-03 eta 0:40:50
epoch [9/50] batch [3/3] time 18.523 (19.349) data 0.006 (0.201) loss 1.0383 (0.9462) acc 75.0000 (79.1667) lr 1.8763e-03 eta 0:39:39
epoch [10/50] batch [1/3] time 21.137 (21.137) data 0.654 (0.654) loss 0.7608 (0.7608) acc 78.1250 (78.1250) lr 1.8763e-03 eta 0:42:58
epoch [10/50] batch [2/3] time 18.358 (19.748) data 0.005 (0.330) loss 0.9615 (0.8611) acc 78.1250 (78.1250) lr 1.8763e-03 eta 0:39:49
epoch [10/50] batch [3/3] time 18.216 (19.237) data 0.008 (0.222) loss 0.4748 (0.7324) acc 84.3750 (80.2083) lr 1.8443e-03 eta 0:38:28
epoch [11/50] batch [1/3] time 20.999 (20.999) data 0.580 (0.580) loss 0.8072 (0.8072) acc 75.0000 (75.0000) lr 1.8443e-03 eta 0:41:38
epoch [11/50] batch [2/3] time 18.291 (19.645) data 0.009 (0.294) loss 0.6465 (0.7269) acc 84.3750 (79.6875) lr 1.8443e-03 eta 0:38:38
epoch [11/50] batch [3/3] time 18.469 (19.253) data 0.007 (0.199) loss 0.8001 (0.7513) acc 75.0000 (78.1250) lr 1.8090e-03 eta 0:37:32
epoch [12/50] batch [1/3] time 21.313 (21.313) data 0.634 (0.634) loss 1.1472 (1.1472) acc 75.0000 (75.0000) lr 1.8090e-03 eta 0:41:12
epoch [12/50] batch [2/3] time 18.671 (19.992) data 0.009 (0.321) loss 0.5908 (0.8690) acc 81.2500 (78.1250) lr 1.8090e-03 eta 0:38:19
epoch [12/50] batch [3/3] time 18.275 (19.420) data 0.008 (0.217) loss 0.9967 (0.9116) acc 78.1250 (78.1250) lr 1.7705e-03 eta 0:36:53
epoch [13/50] batch [1/3] time 21.421 (21.421) data 0.608 (0.608) loss 1.3146 (1.3146) acc 68.7500 (68.7500) lr 1.7705e-03 eta 0:40:20
epoch [13/50] batch [2/3] time 18.126 (19.774) data 0.013 (0.311) loss 0.8870 (1.1008) acc 75.0000 (71.8750) lr 1.7705e-03 eta 0:36:54
epoch [13/50] batch [3/3] time 18.174 (19.240) data 0.007 (0.210) loss 0.6623 (0.9546) acc 81.2500 (75.0000) lr 1.7290e-03 eta 0:35:35
epoch [14/50] batch [1/3] time 20.984 (20.984) data 0.676 (0.676) loss 0.9794 (0.9794) acc 75.0000 (75.0000) lr 1.7290e-03 eta 0:38:28
epoch [14/50] batch [2/3] time 18.098 (19.541) data 0.008 (0.342) loss 0.6715 (0.8254) acc 81.2500 (78.1250) lr 1.7290e-03 eta 0:35:30
epoch [14/50] batch [3/3] time 18.065 (19.049) data 0.006 (0.230) loss 0.6165 (0.7558) acc 81.2500 (79.1667) lr 1.6845e-03 eta 0:34:17
epoch [15/50] batch [1/3] time 20.988 (20.988) data 0.623 (0.623) loss 0.8073 (0.8073) acc 75.0000 (75.0000) lr 1.6845e-03 eta 0:37:25
epoch [15/50] batch [2/3] time 18.430 (19.709) data 0.006 (0.314) loss 0.5526 (0.6799) acc 78.1250 (76.5625) lr 1.6845e-03 eta 0:34:49
epoch [15/50] batch [3/3] time 18.220 (19.213) data 0.009 (0.212) loss 1.0511 (0.8037) acc 78.1250 (77.0833) lr 1.6374e-03 eta 0:33:37
epoch [16/50] batch [1/3] time 20.968 (20.968) data 0.679 (0.679) loss 0.9224 (0.9224) acc 78.1250 (78.1250) lr 1.6374e-03 eta 0:36:20
epoch [16/50] batch [2/3] time 18.347 (19.658) data 0.010 (0.345) loss 1.0483 (0.9854) acc 71.8750 (75.0000) lr 1.6374e-03 eta 0:33:44
epoch [16/50] batch [3/3] time 18.300 (19.205) data 0.007 (0.232) loss 0.6332 (0.8680) acc 84.3750 (78.1250) lr 1.5878e-03 eta 0:32:38
epoch [17/50] batch [1/3] time 21.552 (21.552) data 0.588 (0.588) loss 0.6160 (0.6160) acc 78.1250 (78.1250) lr 1.5878e-03 eta 0:36:16
epoch [17/50] batch [2/3] time 18.307 (19.929) data 0.007 (0.297) loss 0.6588 (0.6374) acc 84.3750 (81.2500) lr 1.5878e-03 eta 0:33:12
epoch [17/50] batch [3/3] time 18.174 (19.344) data 0.009 (0.201) loss 0.7643 (0.6797) acc 87.5000 (83.3333) lr 1.5358e-03 eta 0:31:55
epoch [18/50] batch [1/3] time 21.007 (21.007) data 0.681 (0.681) loss 0.6131 (0.6131) acc 78.1250 (78.1250) lr 1.5358e-03 eta 0:34:18
epoch [18/50] batch [2/3] time 18.921 (19.964) data 0.008 (0.345) loss 0.5467 (0.5799) acc 78.1250 (78.1250) lr 1.5358e-03 eta 0:32:16
epoch [18/50] batch [3/3] time 18.120 (19.349) data 0.009 (0.233) loss 1.3700 (0.8433) acc 62.5000 (72.9167) lr 1.4818e-03 eta 0:30:57
epoch [19/50] batch [1/3] time 21.089 (21.089) data 0.638 (0.638) loss 1.0325 (1.0325) acc 81.2500 (81.2500) lr 1.4818e-03 eta 0:33:23
epoch [19/50] batch [2/3] time 18.584 (19.836) data 0.006 (0.322) loss 0.8809 (0.9567) acc 71.8750 (76.5625) lr 1.4818e-03 eta 0:31:04
epoch [19/50] batch [3/3] time 18.281 (19.318) data 0.007 (0.217) loss 1.0100 (0.9745) acc 71.8750 (75.0000) lr 1.4258e-03 eta 0:29:56
epoch [20/50] batch [1/3] time 20.965 (20.965) data 0.688 (0.688) loss 1.1886 (1.1886) acc 71.8750 (71.8750) lr 1.4258e-03 eta 0:32:08
epoch [20/50] batch [2/3] time 18.229 (19.597) data 0.006 (0.347) loss 1.2434 (1.2160) acc 68.7500 (70.3125) lr 1.4258e-03 eta 0:29:43
epoch [20/50] batch [3/3] time 18.649 (19.281) data 0.009 (0.234) loss 0.8871 (1.1064) acc 75.0000 (71.8750) lr 1.3681e-03 eta 0:28:55
epoch [21/50] batch [1/3] time 21.446 (21.446) data 0.826 (0.826) loss 0.8337 (0.8337) acc 75.0000 (75.0000) lr 1.3681e-03 eta 0:31:48
epoch [21/50] batch [2/3] time 18.277 (19.861) data 0.006 (0.416) loss 1.0476 (0.9407) acc 75.0000 (75.0000) lr 1.3681e-03 eta 0:29:07
epoch [21/50] batch [3/3] time 18.313 (19.345) data 0.010 (0.281) loss 0.5898 (0.8237) acc 84.3750 (78.1250) lr 1.3090e-03 eta 0:28:03
epoch [22/50] batch [1/3] time 22.290 (22.290) data 0.678 (0.678) loss 0.7298 (0.7298) acc 84.3750 (84.3750) lr 1.3090e-03 eta 0:31:56
epoch [22/50] batch [2/3] time 18.286 (20.288) data 0.012 (0.345) loss 0.2610 (0.4954) acc 90.6250 (87.5000) lr 1.3090e-03 eta 0:28:44
epoch [22/50] batch [3/3] time 18.474 (19.683) data 0.007 (0.232) loss 1.0708 (0.6872) acc 75.0000 (83.3333) lr 1.2487e-03 eta 0:27:33
epoch [23/50] batch [1/3] time 21.161 (21.161) data 0.684 (0.684) loss 0.6701 (0.6701) acc 84.3750 (84.3750) lr 1.2487e-03 eta 0:29:16
epoch [23/50] batch [2/3] time 18.258 (19.710) data 0.005 (0.345) loss 1.0502 (0.8602) acc 75.0000 (79.6875) lr 1.2487e-03 eta 0:26:56
epoch [23/50] batch [3/3] time 18.249 (19.223) data 0.007 (0.232) loss 1.1722 (0.9642) acc 71.8750 (77.0833) lr 1.1874e-03 eta 0:25:57
epoch [24/50] batch [1/3] time 21.465 (21.465) data 0.803 (0.803) loss 0.4509 (0.4509) acc 93.7500 (93.7500) lr 1.1874e-03 eta 0:28:37
epoch [24/50] batch [2/3] time 18.411 (19.938) data 0.011 (0.407) loss 1.2323 (0.8416) acc 65.6250 (79.6875) lr 1.1874e-03 eta 0:26:15
epoch [24/50] batch [3/3] time 18.237 (19.371) data 0.004 (0.273) loss 1.0662 (0.9165) acc 81.2500 (80.2083) lr 1.1253e-03 eta 0:25:10
epoch [25/50] batch [1/3] time 21.079 (21.079) data 0.779 (0.779) loss 0.9902 (0.9902) acc 71.8750 (71.8750) lr 1.1253e-03 eta 0:27:03
epoch [25/50] batch [2/3] time 18.319 (19.699) data 0.013 (0.396) loss 1.0914 (1.0408) acc 62.5000 (67.1875) lr 1.1253e-03 eta 0:24:57
epoch [25/50] batch [3/3] time 18.340 (19.246) data 0.005 (0.266) loss 1.0558 (1.0458) acc 78.1250 (70.8333) lr 1.0628e-03 eta 0:24:03
epoch [26/50] batch [1/3] time 21.327 (21.327) data 0.880 (0.880) loss 0.7313 (0.7313) acc 75.0000 (75.0000) lr 1.0628e-03 eta 0:26:18
epoch [26/50] batch [2/3] time 18.295 (19.811) data 0.006 (0.443) loss 0.5350 (0.6332) acc 81.2500 (78.1250) lr 1.0628e-03 eta 0:24:06
epoch [26/50] batch [3/3] time 18.729 (19.450) data 0.008 (0.298) loss 0.9741 (0.7468) acc 71.8750 (76.0417) lr 1.0000e-03 eta 0:23:20
epoch [27/50] batch [1/3] time 21.682 (21.682) data 0.645 (0.645) loss 0.7952 (0.7952) acc 81.2500 (81.2500) lr 1.0000e-03 eta 0:25:39
epoch [27/50] batch [2/3] time 19.626 (20.654) data 0.006 (0.325) loss 0.5886 (0.6919) acc 87.5000 (84.3750) lr 1.0000e-03 eta 0:24:05
epoch [27/50] batch [3/3] time 18.579 (19.962) data 0.015 (0.222) loss 1.1305 (0.8381) acc 71.8750 (80.2083) lr 9.3721e-04 eta 0:22:57
epoch [28/50] batch [1/3] time 21.262 (21.262) data 0.663 (0.663) loss 0.6147 (0.6147) acc 87.5000 (87.5000) lr 9.3721e-04 eta 0:24:05
epoch [28/50] batch [2/3] time 18.553 (19.907) data 0.009 (0.336) loss 0.9913 (0.8030) acc 78.1250 (82.8125) lr 9.3721e-04 eta 0:22:13
epoch [28/50] batch [3/3] time 18.473 (19.429) data 0.013 (0.228) loss 0.6666 (0.7575) acc 78.1250 (81.2500) lr 8.7467e-04 eta 0:21:22
epoch [29/50] batch [1/3] time 21.154 (21.154) data 0.657 (0.657) loss 0.5336 (0.5336) acc 84.3750 (84.3750) lr 8.7467e-04 eta 0:22:55
epoch [29/50] batch [2/3] time 18.605 (19.880) data 0.007 (0.332) loss 1.0171 (0.7754) acc 78.1250 (81.2500) lr 8.7467e-04 eta 0:21:12
epoch [29/50] batch [3/3] time 18.389 (19.383) data 0.005 (0.223) loss 0.9328 (0.8278) acc 71.8750 (78.1250) lr 8.1262e-04 eta 0:20:21
epoch [30/50] batch [1/3] time 21.278 (21.278) data 0.682 (0.682) loss 1.0510 (1.0510) acc 75.0000 (75.0000) lr 8.1262e-04 eta 0:21:59
epoch [30/50] batch [2/3] time 18.537 (19.908) data 0.009 (0.345) loss 0.5696 (0.8103) acc 84.3750 (79.6875) lr 8.1262e-04 eta 0:20:14
epoch [30/50] batch [3/3] time 18.349 (19.388) data 0.008 (0.233) loss 1.1203 (0.9136) acc 78.1250 (79.1667) lr 7.5131e-04 eta 0:19:23
epoch [31/50] batch [1/3] time 22.236 (22.236) data 0.901 (0.901) loss 0.6814 (0.6814) acc 87.5000 (87.5000) lr 7.5131e-04 eta 0:21:51
epoch [31/50] batch [2/3] time 18.601 (20.419) data 0.013 (0.457) loss 1.0318 (0.8566) acc 75.0000 (81.2500) lr 7.5131e-04 eta 0:19:44
epoch [31/50] batch [3/3] time 18.288 (19.708) data 0.009 (0.308) loss 0.7740 (0.8290) acc 71.8750 (78.1250) lr 6.9098e-04 eta 0:18:43
epoch [32/50] batch [1/3] time 22.047 (22.047) data 0.912 (0.912) loss 0.8676 (0.8676) acc 68.7500 (68.7500) lr 6.9098e-04 eta 0:20:34
epoch [32/50] batch [2/3] time 18.934 (20.491) data 0.011 (0.461) loss 0.5192 (0.6934) acc 84.3750 (76.5625) lr 6.9098e-04 eta 0:18:46
epoch [32/50] batch [3/3] time 18.555 (19.846) data 0.013 (0.312) loss 0.6342 (0.6737) acc 84.3750 (79.1667) lr 6.3188e-04 eta 0:17:51
epoch [33/50] batch [1/3] time 21.295 (21.295) data 0.738 (0.738) loss 0.6133 (0.6133) acc 81.2500 (81.2500) lr 6.3188e-04 eta 0:18:48
epoch [33/50] batch [2/3] time 18.683 (19.989) data 0.006 (0.372) loss 0.8693 (0.7413) acc 81.2500 (81.2500) lr 6.3188e-04 eta 0:17:19
epoch [33/50] batch [3/3] time 18.448 (19.475) data 0.006 (0.250) loss 1.0098 (0.8308) acc 78.1250 (80.2083) lr 5.7422e-04 eta 0:16:33
epoch [34/50] batch [1/3] time 20.975 (20.975) data 0.535 (0.535) loss 0.7231 (0.7231) acc 81.2500 (81.2500) lr 5.7422e-04 eta 0:17:28
epoch [34/50] batch [2/3] time 18.654 (19.815) data 0.008 (0.271) loss 0.1105 (0.4168) acc 100.0000 (90.6250) lr 5.7422e-04 eta 0:16:10
epoch [34/50] batch [3/3] time 18.570 (19.400) data 0.008 (0.184) loss 1.1430 (0.6589) acc 65.6250 (82.2917) lr 5.1825e-04 eta 0:15:31
epoch [35/50] batch [1/3] time 21.003 (21.003) data 0.614 (0.614) loss 1.1362 (1.1362) acc 68.7500 (68.7500) lr 5.1825e-04 eta 0:16:27
epoch [35/50] batch [2/3] time 18.320 (19.661) data 0.005 (0.309) loss 0.6084 (0.8723) acc 78.1250 (73.4375) lr 5.1825e-04 eta 0:15:04
epoch [35/50] batch [3/3] time 18.268 (19.197) data 0.008 (0.209) loss 0.5843 (0.7763) acc 87.5000 (78.1250) lr 4.6417e-04 eta 0:14:23
epoch [36/50] batch [1/3] time 21.235 (21.235) data 0.683 (0.683) loss 1.0496 (1.0496) acc 75.0000 (75.0000) lr 4.6417e-04 eta 0:15:34
epoch [36/50] batch [2/3] time 18.412 (19.824) data 0.007 (0.345) loss 0.9751 (1.0123) acc 71.8750 (73.4375) lr 4.6417e-04 eta 0:14:12
epoch [36/50] batch [3/3] time 18.245 (19.297) data 0.006 (0.232) loss 0.5682 (0.8643) acc 87.5000 (78.1250) lr 4.1221e-04 eta 0:13:30
epoch [37/50] batch [1/3] time 21.665 (21.665) data 0.658 (0.658) loss 1.1144 (1.1144) acc 71.8750 (71.8750) lr 4.1221e-04 eta 0:14:48
epoch [37/50] batch [2/3] time 18.729 (20.197) data 0.005 (0.331) loss 0.6748 (0.8946) acc 78.1250 (75.0000) lr 4.1221e-04 eta 0:13:27
epoch [37/50] batch [3/3] time 18.324 (19.573) data 0.026 (0.230) loss 0.7615 (0.8503) acc 75.0000 (75.0000) lr 3.6258e-04 eta 0:12:43
epoch [38/50] batch [1/3] time 21.188 (21.188) data 0.686 (0.686) loss 0.7129 (0.7129) acc 78.1250 (78.1250) lr 3.6258e-04 eta 0:13:25
epoch [38/50] batch [2/3] time 18.227 (19.707) data 0.005 (0.345) loss 0.7855 (0.7492) acc 81.2500 (79.6875) lr 3.6258e-04 eta 0:12:09
epoch [38/50] batch [3/3] time 18.431 (19.282) data 0.007 (0.233) loss 0.7551 (0.7512) acc 78.1250 (79.1667) lr 3.1545e-04 eta 0:11:34
epoch [39/50] batch [1/3] time 21.193 (21.193) data 0.656 (0.656) loss 0.7745 (0.7745) acc 81.2500 (81.2500) lr 3.1545e-04 eta 0:12:21
epoch [39/50] batch [2/3] time 18.566 (19.880) data 0.005 (0.331) loss 0.8265 (0.8005) acc 75.0000 (78.1250) lr 3.1545e-04 eta 0:11:15
epoch [39/50] batch [3/3] time 18.207 (19.322) data 0.005 (0.222) loss 0.7161 (0.7724) acc 81.2500 (79.1667) lr 2.7103e-04 eta 0:10:37
epoch [40/50] batch [1/3] time 21.080 (21.080) data 0.706 (0.706) loss 0.7937 (0.7937) acc 78.1250 (78.1250) lr 2.7103e-04 eta 0:11:14
epoch [40/50] batch [2/3] time 18.166 (19.623) data 0.007 (0.357) loss 0.8517 (0.8227) acc 81.2500 (79.6875) lr 2.7103e-04 eta 0:10:08
epoch [40/50] batch [3/3] time 18.358 (19.201) data 0.006 (0.240) loss 1.6028 (1.0827) acc 59.3750 (72.9167) lr 2.2949e-04 eta 0:09:36
epoch [41/50] batch [1/3] time 21.227 (21.227) data 0.580 (0.580) loss 0.4854 (0.4854) acc 87.5000 (87.5000) lr 2.2949e-04 eta 0:10:15
epoch [41/50] batch [2/3] time 18.331 (19.779) data 0.009 (0.295) loss 0.7555 (0.6204) acc 78.1250 (82.8125) lr 2.2949e-04 eta 0:09:13
epoch [41/50] batch [3/3] time 18.178 (19.245) data 0.009 (0.199) loss 1.3667 (0.8692) acc 71.8750 (79.1667) lr 1.9098e-04 eta 0:08:39
epoch [42/50] batch [1/3] time 21.227 (21.227) data 0.723 (0.723) loss 1.3216 (1.3216) acc 75.0000 (75.0000) lr 1.9098e-04 eta 0:09:11
epoch [42/50] batch [2/3] time 18.577 (19.902) data 0.013 (0.368) loss 0.5665 (0.9441) acc 81.2500 (78.1250) lr 1.9098e-04 eta 0:08:17
epoch [42/50] batch [3/3] time 18.361 (19.388) data 0.005 (0.247) loss 0.8603 (0.9161) acc 75.0000 (77.0833) lr 1.5567e-04 eta 0:07:45
epoch [43/50] batch [1/3] time 21.904 (21.904) data 0.893 (0.893) loss 1.1647 (1.1647) acc 68.7500 (68.7500) lr 1.5567e-04 eta 0:08:23
epoch [43/50] batch [2/3] time 18.176 (20.040) data 0.006 (0.449) loss 0.4911 (0.8279) acc 93.7500 (81.2500) lr 1.5567e-04 eta 0:07:20
epoch [43/50] batch [3/3] time 18.193 (19.424) data 0.008 (0.302) loss 0.5529 (0.7362) acc 87.5000 (83.3333) lr 1.2369e-04 eta 0:06:47
epoch [44/50] batch [1/3] time 21.224 (21.224) data 0.704 (0.704) loss 1.0069 (1.0069) acc 68.7500 (68.7500) lr 1.2369e-04 eta 0:07:04
epoch [44/50] batch [2/3] time 18.313 (19.769) data 0.005 (0.355) loss 0.9092 (0.9580) acc 68.7500 (68.7500) lr 1.2369e-04 eta 0:06:15
epoch [44/50] batch [3/3] time 18.235 (19.257) data 0.010 (0.240) loss 0.7250 (0.8804) acc 78.1250 (71.8750) lr 9.5173e-05 eta 0:05:46
epoch [45/50] batch [1/3] time 21.364 (21.364) data 0.650 (0.650) loss 1.1871 (1.1871) acc 75.0000 (75.0000) lr 9.5173e-05 eta 0:06:03
epoch [45/50] batch [2/3] time 18.420 (19.892) data 0.004 (0.327) loss 0.8196 (1.0034) acc 71.8750 (73.4375) lr 9.5173e-05 eta 0:05:18
epoch [45/50] batch [3/3] time 18.388 (19.391) data 0.006 (0.220) loss 0.9430 (0.9832) acc 71.8750 (72.9167) lr 7.0224e-05 eta 0:04:50
epoch [46/50] batch [1/3] time 21.829 (21.829) data 0.967 (0.967) loss 0.9194 (0.9194) acc 68.7500 (68.7500) lr 7.0224e-05 eta 0:05:05
epoch [46/50] batch [2/3] time 18.478 (20.153) data 0.008 (0.488) loss 0.9597 (0.9396) acc 75.0000 (71.8750) lr 7.0224e-05 eta 0:04:21
epoch [46/50] batch [3/3] time 18.245 (19.517) data 0.007 (0.328) loss 1.0122 (0.9638) acc 78.1250 (73.9583) lr 4.8943e-05 eta 0:03:54
epoch [47/50] batch [1/3] time 21.510 (21.510) data 0.798 (0.798) loss 0.6722 (0.6722) acc 84.3750 (84.3750) lr 4.8943e-05 eta 0:03:56
epoch [47/50] batch [2/3] time 18.406 (19.958) data 0.008 (0.403) loss 0.8057 (0.7390) acc 84.3750 (84.3750) lr 4.8943e-05 eta 0:03:19
epoch [47/50] batch [3/3] time 18.880 (19.599) data 0.004 (0.270) loss 1.0938 (0.8572) acc 75.0000 (81.2500) lr 3.1417e-05 eta 0:02:56
epoch [48/50] batch [1/3] time 21.366 (21.366) data 0.764 (0.764) loss 0.5723 (0.5723) acc 87.5000 (87.5000) lr 3.1417e-05 eta 0:02:50
epoch [48/50] batch [2/3] time 18.286 (19.826) data 0.014 (0.389) loss 0.7218 (0.6471) acc 75.0000 (81.2500) lr 3.1417e-05 eta 0:02:18
epoch [48/50] batch [3/3] time 18.354 (19.335) data 0.005 (0.261) loss 0.7046 (0.6662) acc 71.8750 (78.1250) lr 1.7713e-05 eta 0:01:56
epoch [49/50] batch [1/3] time 21.411 (21.411) data 0.799 (0.799) loss 0.8394 (0.8394) acc 71.8750 (71.8750) lr 1.7713e-05 eta 0:01:47
epoch [49/50] batch [2/3] time 18.451 (19.931) data 0.010 (0.404) loss 1.0802 (0.9598) acc 68.7500 (70.3125) lr 1.7713e-05 eta 0:01:19
epoch [49/50] batch [3/3] time 18.373 (19.411) data 0.008 (0.272) loss 1.0877 (1.0024) acc 78.1250 (72.9167) lr 7.8853e-06 eta 0:00:58
epoch [50/50] batch [1/3] time 21.459 (21.459) data 0.870 (0.870) loss 1.2261 (1.2261) acc 71.8750 (71.8750) lr 7.8853e-06 eta 0:00:42
epoch [50/50] batch [2/3] time 19.264 (20.362) data 0.008 (0.439) loss 0.8259 (1.0260) acc 84.3750 (78.1250) lr 7.8853e-06 eta 0:00:20
epoch [50/50] batch [3/3] time 19.645 (20.123) data 0.005 (0.294) loss 0.6795 (0.9105) acc 78.1250 (78.1250) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/base2new/train_base/caltech101/shots_1_8.0/ResidualPrompting/vit_b16_ep50_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 2,465
* correct: 2,296
* accuracy: 93.1%
* error: 6.9%
* macro_f1: 90.5%
Elapsed: 1:02:44
