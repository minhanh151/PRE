***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/ResidualPrompting/vit_b16_ep50_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '8.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/base2new/train_base/oxford_flowers/shots_16_8.0/ResidualPrompting/vit_b16_ep50_ctxv1/seed1
resume: 
root: /home/ducan/Downloads/MinhAnh/CoOp/DATA
seed: 1
source_domains: None
target_domains: None
trainer: ResidualPrompting
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /home/ducan/Downloads/MinhAnh/CoOp/DATA
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16_8.0/ResidualPrompting/vit_b16_ep50_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: amp
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: amp
    W: 8.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: ResidualPrompting
  PLOT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N: 4
    N_CTX: 16
    PREC: amp
  ResidualPrompting:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: amp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.10.2
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.16 (default, Jun 12 2023, 18:09:05)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.17
Is CUDA available: False
CUDA runtime version: 12.2.91
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.2
[pip3] torch==1.10.2
[pip3] torchvision==0.11.3
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h6d00ec8_46342  
[conda] mkl-service               2.4.0            py38h5eee18b_1  
[conda] mkl_fft                   1.3.6            py38h417a72b_1  
[conda] mkl_random                1.2.2            py38h417a72b_1  
[conda] numpy                     1.24.3           py38hf6e8229_1  
[conda] numpy-base                1.24.3           py38h060ed82_1  
[conda] pytorch                   1.10.2          py3.8_cuda10.2_cudnn7.6.5_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.11.3               py38_cu102    pytorch
        Pillow (9.4.0)

Loading trainer: ResidualPrompting
Loading dataset: OxfordFlowers
Reading split from /home/ducan/Downloads/MinhAnh/CoOp/DATA/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /home/ducan/Downloads/MinhAnh/CoOp/DATA/oxford_flowers/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,053
---------  -------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Using skip connection in MLP
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16_8.0/ResidualPrompting/vit_b16_ep50_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/25] time 12.743 (20.177) data 0.003 (0.202) loss 4.8997 (5.0148) acc 15.6250 (14.3750) lr 1.0000e-05 eta 6:58:40
epoch [1/50] batch [10/25] time 12.943 (16.659) data 0.007 (0.104) loss 4.9449 (4.7462) acc 12.5000 (18.1250) lr 1.0000e-05 eta 5:44:17
epoch [1/50] batch [15/25] time 13.223 (16.105) data 0.004 (0.071) loss 4.6308 (4.7765) acc 21.8750 (18.5417) lr 1.0000e-05 eta 5:31:30
epoch [1/50] batch [20/25] time 33.060 (16.335) data 0.005 (0.054) loss 4.1867 (4.7460) acc 31.2500 (20.0000) lr 1.0000e-05 eta 5:34:51
epoch [1/50] batch [25/25] time 13.045 (16.108) data 0.005 (0.056) loss 3.9470 (4.7123) acc 28.1250 (20.6250) lr 2.0000e-03 eta 5:28:52
epoch [2/50] batch [5/25] time 13.269 (16.684) data 0.013 (0.235) loss 1.4983 (2.6122) acc 65.6250 (50.6250) lr 2.0000e-03 eta 5:39:14
epoch [2/50] batch [10/25] time 12.969 (14.912) data 0.004 (0.120) loss 1.5105 (2.1234) acc 68.7500 (57.8125) lr 2.0000e-03 eta 5:01:57
epoch [2/50] batch [15/25] time 12.978 (14.274) data 0.004 (0.082) loss 1.2885 (2.0325) acc 71.8750 (58.3333) lr 2.0000e-03 eta 4:47:52
epoch [2/50] batch [20/25] time 13.018 (13.973) data 0.006 (0.063) loss 1.9038 (1.9102) acc 59.3750 (59.5312) lr 2.0000e-03 eta 4:40:37
epoch [2/50] batch [25/25] time 12.980 (13.791) data 0.006 (0.051) loss 1.5839 (1.8500) acc 65.6250 (60.2500) lr 1.9980e-03 eta 4:35:49
epoch [3/50] batch [5/25] time 13.153 (13.762) data 0.005 (0.238) loss 1.3792 (1.5525) acc 59.3750 (64.3750) lr 1.9980e-03 eta 4:34:05
epoch [3/50] batch [10/25] time 12.963 (14.801) data 0.004 (0.122) loss 1.6196 (1.4994) acc 59.3750 (64.0625) lr 1.9980e-03 eta 4:53:33
epoch [3/50] batch [15/25] time 12.945 (14.188) data 0.004 (0.083) loss 1.4550 (1.4886) acc 53.1250 (63.3333) lr 1.9980e-03 eta 4:40:12
epoch [3/50] batch [20/25] time 11.764 (18.397) data 0.005 (0.077) loss 1.3087 (1.4724) acc 68.7500 (62.8125) lr 1.9980e-03 eta 6:01:48
epoch [3/50] batch [25/25] time 12.425 (18.377) data 0.005 (0.063) loss 1.1222 (1.4600) acc 56.2500 (62.5000) lr 1.9921e-03 eta 5:59:53
epoch [4/50] batch [5/25] time 13.051 (13.650) data 0.004 (0.246) loss 1.6044 (1.4011) acc 53.1250 (58.7500) lr 1.9921e-03 eta 4:26:10
epoch [4/50] batch [10/25] time 12.936 (13.366) data 0.004 (0.127) loss 1.0694 (1.3549) acc 68.7500 (62.1875) lr 1.9921e-03 eta 4:19:31
epoch [4/50] batch [15/25] time 13.088 (13.253) data 0.004 (0.086) loss 1.1608 (1.3182) acc 62.5000 (63.9583) lr 1.9921e-03 eta 4:16:13
epoch [4/50] batch [20/25] time 31.809 (15.728) data 0.016 (0.067) loss 1.4422 (1.3660) acc 62.5000 (62.6562) lr 1.9921e-03 eta 5:02:45
epoch [4/50] batch [25/25] time 12.997 (15.110) data 0.005 (0.054) loss 1.1133 (1.3513) acc 71.8750 (62.7500) lr 1.9823e-03 eta 4:49:36
epoch [5/50] batch [5/25] time 12.846 (19.844) data 0.003 (0.188) loss 1.3799 (1.2128) acc 65.6250 (65.6250) lr 1.9823e-03 eta 6:18:41
epoch [5/50] batch [10/25] time 12.735 (21.306) data 0.010 (0.097) loss 2.2296 (1.3720) acc 40.6250 (61.8750) lr 1.9823e-03 eta 6:44:49
epoch [5/50] batch [15/25] time 12.960 (18.534) data 0.006 (0.066) loss 1.3706 (1.3720) acc 65.6250 (62.2917) lr 1.9823e-03 eta 5:50:36
epoch [5/50] batch [20/25] time 12.964 (17.141) data 0.006 (0.051) loss 1.3064 (1.3513) acc 71.8750 (63.1250) lr 1.9823e-03 eta 5:22:48
epoch [5/50] batch [25/25] time 12.552 (18.990) data 0.005 (0.042) loss 1.0260 (1.3129) acc 65.6250 (63.8750) lr 1.9686e-03 eta 5:56:04
epoch [6/50] batch [5/25] time 13.084 (13.744) data 0.004 (0.253) loss 1.5943 (1.3062) acc 62.5000 (67.5000) lr 1.9686e-03 eta 4:16:33
epoch [6/50] batch [10/25] time 12.893 (16.485) data 0.005 (0.141) loss 1.4428 (1.2661) acc 65.6250 (65.0000) lr 1.9686e-03 eta 5:06:20
epoch [6/50] batch [15/25] time 12.995 (15.318) data 0.004 (0.095) loss 1.4716 (1.2687) acc 53.1250 (65.0000) lr 1.9686e-03 eta 4:43:22
epoch [6/50] batch [20/25] time 12.965 (14.744) data 0.006 (0.073) loss 1.3657 (1.2616) acc 65.6250 (64.0625) lr 1.9686e-03 eta 4:31:31
epoch [6/50] batch [25/25] time 12.967 (14.388) data 0.006 (0.060) loss 0.9833 (1.2234) acc 65.6250 (64.8750) lr 1.9511e-03 eta 4:23:47
epoch [7/50] batch [5/25] time 13.143 (16.468) data 0.004 (0.408) loss 1.1239 (1.3493) acc 75.0000 (61.8750) lr 1.9511e-03 eta 5:00:32
epoch [7/50] batch [10/25] time 12.958 (14.775) data 0.004 (0.207) loss 1.1729 (1.2954) acc 68.7500 (64.0625) lr 1.9511e-03 eta 4:28:25
epoch [7/50] batch [15/25] time 13.006 (15.059) data 0.004 (0.139) loss 1.2069 (1.2311) acc 68.7500 (65.2083) lr 1.9511e-03 eta 4:32:19
epoch [7/50] batch [20/25] time 12.937 (14.545) data 0.006 (0.106) loss 1.1074 (1.1938) acc 75.0000 (66.7188) lr 1.9511e-03 eta 4:21:48
epoch [7/50] batch [25/25] time 27.784 (15.566) data 0.005 (0.087) loss 1.2796 (1.2178) acc 62.5000 (65.3750) lr 1.9298e-03 eta 4:38:53
epoch [8/50] batch [5/25] time 47.757 (25.632) data 0.013 (2.734) loss 0.8992 (1.1217) acc 75.0000 (68.7500) lr 1.9298e-03 eta 7:37:06
epoch [8/50] batch [10/25] time 12.943 (21.331) data 0.003 (1.372) loss 1.1266 (1.1197) acc 68.7500 (69.3750) lr 1.9298e-03 eta 6:18:37
epoch [8/50] batch [15/25] time 12.926 (18.538) data 0.004 (0.917) loss 1.0210 (1.1433) acc 81.2500 (68.7500) lr 1.9298e-03 eta 5:27:30
epoch [8/50] batch [20/25] time 12.668 (18.940) data 0.005 (0.690) loss 0.8761 (1.1715) acc 68.7500 (66.8750) lr 1.9298e-03 eta 5:33:01
epoch [8/50] batch [25/25] time 12.662 (18.930) data 0.005 (0.554) loss 0.9097 (1.1954) acc 78.1250 (66.5000) lr 1.9048e-03 eta 5:31:16
epoch [9/50] batch [5/25] time 13.189 (13.791) data 0.022 (0.262) loss 1.2358 (1.1888) acc 59.3750 (63.7500) lr 1.9048e-03 eta 4:00:11
epoch [9/50] batch [10/25] time 13.811 (13.586) data 0.004 (0.136) loss 1.4359 (1.2372) acc 53.1250 (63.7500) lr 1.9048e-03 eta 3:55:29
epoch [9/50] batch [15/25] time 13.047 (15.746) data 0.003 (0.092) loss 1.0363 (1.1411) acc 71.8750 (67.5000) lr 1.9048e-03 eta 4:31:37
epoch [9/50] batch [20/25] time 12.204 (16.528) data 0.005 (0.071) loss 1.2644 (1.1564) acc 62.5000 (66.7188) lr 1.9048e-03 eta 4:43:44
epoch [9/50] batch [25/25] time 12.983 (16.347) data 0.005 (0.059) loss 0.9300 (1.1746) acc 78.1250 (66.0000) lr 1.8763e-03 eta 4:39:16
epoch [10/50] batch [5/25] time 13.087 (13.713) data 0.004 (0.243) loss 1.3282 (1.3536) acc 53.1250 (58.7500) lr 1.8763e-03 eta 3:53:07
epoch [10/50] batch [10/25] time 12.966 (13.426) data 0.008 (0.125) loss 1.1680 (1.2604) acc 68.7500 (62.8125) lr 1.8763e-03 eta 3:47:07
epoch [10/50] batch [15/25] time 13.016 (13.302) data 0.006 (0.085) loss 1.3353 (1.2531) acc 59.3750 (63.3333) lr 1.8763e-03 eta 3:43:54
epoch [10/50] batch [20/25] time 22.634 (15.160) data 0.005 (0.065) loss 0.7796 (1.1889) acc 71.8750 (64.2188) lr 1.8763e-03 eta 4:13:56
epoch [10/50] batch [25/25] time 12.826 (16.296) data 0.005 (0.055) loss 1.3884 (1.1918) acc 59.3750 (64.3750) lr 1.8443e-03 eta 4:31:35
epoch [11/50] batch [5/25] time 13.085 (14.812) data 0.009 (0.234) loss 1.2166 (1.2227) acc 62.5000 (64.3750) lr 1.8443e-03 eta 4:05:37
epoch [11/50] batch [10/25] time 13.150 (13.965) data 0.013 (0.121) loss 1.2518 (1.1983) acc 62.5000 (64.6875) lr 1.8443e-03 eta 3:50:25
epoch [11/50] batch [15/25] time 12.563 (17.416) data 0.003 (0.082) loss 0.9083 (1.1501) acc 75.0000 (65.8333) lr 1.8443e-03 eta 4:45:55
epoch [11/50] batch [20/25] time 12.638 (17.957) data 0.006 (0.063) loss 1.1552 (1.1757) acc 71.8750 (65.7812) lr 1.8443e-03 eta 4:53:17
epoch [11/50] batch [25/25] time 13.052 (16.967) data 0.005 (0.051) loss 1.4195 (1.1985) acc 65.6250 (65.6250) lr 1.8090e-03 eta 4:35:43
epoch [12/50] batch [5/25] time 13.102 (13.740) data 0.004 (0.227) loss 1.2855 (1.1560) acc 56.2500 (64.3750) lr 1.8090e-03 eta 3:42:07
epoch [12/50] batch [10/25] time 12.673 (18.399) data 0.005 (0.119) loss 1.5943 (1.2180) acc 53.1250 (62.8125) lr 1.8090e-03 eta 4:55:54
epoch [12/50] batch [15/25] time 12.864 (18.173) data 0.005 (0.082) loss 1.0070 (1.1821) acc 68.7500 (63.9583) lr 1.8090e-03 eta 4:50:46
epoch [12/50] batch [20/25] time 12.944 (16.875) data 0.006 (0.063) loss 1.2175 (1.1671) acc 71.8750 (64.5312) lr 1.8090e-03 eta 4:28:35
epoch [12/50] batch [25/25] time 12.956 (16.091) data 0.005 (0.052) loss 1.5511 (1.1754) acc 62.5000 (64.3750) lr 1.7705e-03 eta 4:14:46
epoch [13/50] batch [5/25] time 12.232 (19.632) data 0.008 (0.224) loss 0.8709 (1.0586) acc 75.0000 (68.7500) lr 1.7705e-03 eta 5:09:11
epoch [13/50] batch [10/25] time 13.065 (16.381) data 0.007 (0.115) loss 1.1650 (1.0535) acc 71.8750 (68.4375) lr 1.7705e-03 eta 4:16:37
epoch [13/50] batch [15/25] time 13.035 (15.284) data 0.004 (0.078) loss 0.9582 (1.0575) acc 68.7500 (67.7083) lr 1.7705e-03 eta 3:58:10
epoch [13/50] batch [20/25] time 24.959 (15.493) data 0.005 (0.060) loss 1.4229 (1.0825) acc 62.5000 (67.5000) lr 1.7705e-03 eta 4:00:08
epoch [13/50] batch [25/25] time 13.069 (16.252) data 0.006 (0.049) loss 1.2762 (1.1013) acc 53.1250 (67.0000) lr 1.7290e-03 eta 4:10:33
epoch [14/50] batch [5/25] time 12.507 (23.680) data 0.008 (0.255) loss 1.1255 (1.0672) acc 68.7500 (66.8750) lr 1.7290e-03 eta 6:03:05
epoch [14/50] batch [10/25] time 11.721 (24.686) data 0.003 (0.135) loss 0.9271 (1.1033) acc 71.8750 (65.9375) lr 1.7290e-03 eta 6:16:28
epoch [14/50] batch [15/25] time 30.833 (22.290) data 0.003 (0.092) loss 1.4547 (1.1582) acc 53.1250 (64.7917) lr 1.7290e-03 eta 5:38:03
epoch [14/50] batch [20/25] time 23.439 (22.535) data 0.043 (0.073) loss 1.3473 (1.1751) acc 53.1250 (63.9062) lr 1.7290e-03 eta 5:39:54
epoch [14/50] batch [25/25] time 14.656 (21.674) data 0.023 (0.060) loss 0.9319 (1.1562) acc 75.0000 (64.7500) lr 1.6845e-03 eta 5:25:06
epoch [15/50] batch [5/25] time 13.296 (13.621) data 0.004 (0.268) loss 1.2368 (0.9536) acc 59.3750 (72.5000) lr 1.6845e-03 eta 3:23:11
epoch [15/50] batch [10/25] time 12.965 (14.672) data 0.004 (0.137) loss 1.2929 (1.1278) acc 68.7500 (67.5000) lr 1.6845e-03 eta 3:37:38
epoch [15/50] batch [15/25] time 12.978 (14.113) data 0.004 (0.093) loss 1.0214 (1.0897) acc 65.6250 (68.3333) lr 1.6845e-03 eta 3:28:10
epoch [15/50] batch [20/25] time 12.970 (13.836) data 0.006 (0.071) loss 1.0068 (1.0767) acc 71.8750 (68.2812) lr 1.6845e-03 eta 3:22:55
epoch [15/50] batch [25/25] time 27.566 (15.340) data 0.005 (0.059) loss 1.1214 (1.0472) acc 62.5000 (69.1250) lr 1.6374e-03 eta 3:43:42
epoch [16/50] batch [5/25] time 13.007 (22.291) data 0.004 (0.243) loss 0.9892 (1.0942) acc 71.8750 (65.6250) lr 1.6374e-03 eta 5:23:13
epoch [16/50] batch [10/25] time 12.702 (21.241) data 0.004 (0.125) loss 0.9130 (1.0641) acc 75.0000 (67.5000) lr 1.6374e-03 eta 5:06:13
epoch [16/50] batch [15/25] time 12.843 (20.560) data 0.004 (0.086) loss 1.4592 (1.0875) acc 59.3750 (67.0833) lr 1.6374e-03 eta 4:54:41
epoch [16/50] batch [20/25] time 12.910 (18.667) data 0.005 (0.066) loss 1.2440 (1.0634) acc 68.7500 (67.6562) lr 1.6374e-03 eta 4:26:00
epoch [16/50] batch [25/25] time 12.936 (17.531) data 0.006 (0.054) loss 1.2352 (1.0740) acc 68.7500 (67.7500) lr 1.5878e-03 eta 4:08:21
epoch [17/50] batch [5/25] time 13.109 (13.736) data 0.005 (0.253) loss 1.1640 (0.9978) acc 56.2500 (68.7500) lr 1.5878e-03 eta 3:13:26
epoch [17/50] batch [10/25] time 13.025 (13.424) data 0.010 (0.130) loss 0.9658 (1.0463) acc 68.7500 (67.5000) lr 1.5878e-03 eta 3:07:56
epoch [17/50] batch [15/25] time 12.963 (13.288) data 0.004 (0.088) loss 0.8111 (1.0319) acc 78.1250 (67.5000) lr 1.5878e-03 eta 3:04:55
epoch [17/50] batch [20/25] time 12.978 (13.216) data 0.005 (0.068) loss 0.8303 (1.0168) acc 75.0000 (67.6562) lr 1.5878e-03 eta 3:02:49
epoch [17/50] batch [25/25] time 12.967 (13.167) data 0.006 (0.056) loss 1.4802 (1.0747) acc 56.2500 (65.8750) lr 1.5358e-03 eta 3:01:02
epoch [18/50] batch [5/25] time 12.765 (27.383) data 0.005 (0.270) loss 0.8751 (1.1335) acc 78.1250 (64.3750) lr 1.5358e-03 eta 6:14:14
epoch [18/50] batch [10/25] time 12.686 (23.263) data 0.006 (0.138) loss 0.7263 (1.0782) acc 81.2500 (66.2500) lr 1.5358e-03 eta 5:15:59
epoch [18/50] batch [15/25] time 12.763 (21.637) data 0.005 (0.094) loss 0.8851 (1.0728) acc 65.6250 (66.6667) lr 1.5358e-03 eta 4:52:06
epoch [18/50] batch [20/25] time 12.973 (19.471) data 0.006 (0.072) loss 1.3784 (1.0545) acc 53.1250 (66.5625) lr 1.5358e-03 eta 4:21:14
epoch [18/50] batch [25/25] time 12.847 (19.394) data 0.006 (0.060) loss 1.3207 (1.0470) acc 59.3750 (66.8750) lr 1.4818e-03 eta 4:18:35
epoch [19/50] batch [5/25] time 13.102 (13.734) data 0.004 (0.235) loss 0.5813 (0.9532) acc 84.3750 (70.0000) lr 1.4818e-03 eta 3:01:58
epoch [19/50] batch [10/25] time 12.976 (13.411) data 0.011 (0.121) loss 0.8911 (0.9682) acc 71.8750 (71.2500) lr 1.4818e-03 eta 2:56:34
epoch [19/50] batch [15/25] time 32.906 (17.043) data 0.015 (0.083) loss 0.9064 (1.0151) acc 71.8750 (69.1667) lr 1.4818e-03 eta 3:42:58
epoch [19/50] batch [20/25] time 13.148 (16.660) data 0.005 (0.063) loss 1.1130 (1.0085) acc 62.5000 (69.0625) lr 1.4818e-03 eta 3:36:34
epoch [19/50] batch [25/25] time 12.764 (16.423) data 0.006 (0.055) loss 1.1463 (1.0328) acc 62.5000 (68.5000) lr 1.4258e-03 eta 3:32:07
epoch [20/50] batch [5/25] time 27.626 (16.649) data 0.008 (0.237) loss 1.4446 (1.0937) acc 53.1250 (69.3750) lr 1.4258e-03 eta 3:33:39
epoch [20/50] batch [10/25] time 22.682 (17.558) data 0.014 (0.123) loss 1.1760 (1.1429) acc 65.6250 (67.5000) lr 1.4258e-03 eta 3:43:52
epoch [20/50] batch [15/25] time 12.991 (16.009) data 0.006 (0.084) loss 0.9383 (1.0914) acc 75.0000 (68.3333) lr 1.4258e-03 eta 3:22:46
epoch [20/50] batch [20/25] time 12.962 (15.251) data 0.006 (0.064) loss 1.2912 (1.0763) acc 71.8750 (70.0000) lr 1.4258e-03 eta 3:11:54
epoch [20/50] batch [25/25] time 12.910 (15.557) data 0.006 (0.053) loss 0.7718 (1.0585) acc 71.8750 (70.3750) lr 1.3681e-03 eta 3:14:27
epoch [21/50] batch [5/25] time 13.136 (13.795) data 0.007 (0.272) loss 0.9344 (1.0328) acc 71.8750 (71.2500) lr 1.3681e-03 eta 2:51:17
epoch [21/50] batch [10/25] time 12.956 (13.442) data 0.004 (0.139) loss 1.0529 (1.0580) acc 68.7500 (69.3750) lr 1.3681e-03 eta 2:45:46
epoch [21/50] batch [15/25] time 11.756 (16.628) data 0.006 (0.095) loss 1.1778 (1.0288) acc 65.6250 (70.0000) lr 1.3681e-03 eta 3:23:41
epoch [21/50] batch [20/25] time 13.208 (15.680) data 0.006 (0.072) loss 0.9238 (1.0117) acc 81.2500 (69.6875) lr 1.3681e-03 eta 3:10:46
epoch [21/50] batch [25/25] time 12.972 (15.163) data 0.006 (0.059) loss 1.0000 (0.9889) acc 65.6250 (70.1250) lr 1.3090e-03 eta 3:03:12
epoch [22/50] batch [5/25] time 13.380 (16.414) data 0.006 (0.257) loss 0.6244 (0.9359) acc 75.0000 (71.8750) lr 1.3090e-03 eta 3:16:58
epoch [22/50] batch [10/25] time 13.038 (14.773) data 0.004 (0.131) loss 0.9557 (0.9530) acc 59.3750 (70.6250) lr 1.3090e-03 eta 2:56:02
epoch [22/50] batch [15/25] time 13.160 (14.209) data 0.004 (0.089) loss 0.9622 (0.9455) acc 75.0000 (71.4583) lr 1.3090e-03 eta 2:48:08
epoch [22/50] batch [20/25] time 13.100 (13.952) data 0.006 (0.069) loss 1.2548 (0.9634) acc 62.5000 (70.6250) lr 1.3090e-03 eta 2:43:56
epoch [22/50] batch [25/25] time 12.983 (13.771) data 0.006 (0.056) loss 1.0227 (0.9429) acc 68.7500 (71.6250) lr 1.2487e-03 eta 2:40:39
epoch [23/50] batch [5/25] time 13.110 (13.756) data 0.006 (0.233) loss 1.2838 (0.9644) acc 56.2500 (72.5000) lr 1.2487e-03 eta 2:39:20
epoch [23/50] batch [10/25] time 12.977 (13.449) data 0.008 (0.121) loss 0.6679 (0.8930) acc 81.2500 (72.8125) lr 1.2487e-03 eta 2:34:39
epoch [23/50] batch [15/25] time 13.301 (13.349) data 0.004 (0.083) loss 1.1751 (0.9628) acc 65.6250 (71.4583) lr 1.2487e-03 eta 2:32:24
epoch [23/50] batch [20/25] time 13.083 (13.282) data 0.006 (0.063) loss 1.0811 (0.9703) acc 62.5000 (70.7812) lr 1.2487e-03 eta 2:30:31
epoch [23/50] batch [25/25] time 12.877 (14.381) data 0.006 (0.054) loss 0.9533 (0.9710) acc 68.7500 (70.8750) lr 1.1874e-03 eta 2:41:47
epoch [24/50] batch [5/25] time 13.165 (13.796) data 0.004 (0.259) loss 0.9771 (0.8882) acc 75.0000 (71.8750) lr 1.1874e-03 eta 2:34:03
epoch [24/50] batch [10/25] time 12.981 (13.497) data 0.010 (0.133) loss 0.9038 (0.9111) acc 68.7500 (71.8750) lr 1.1874e-03 eta 2:29:35
epoch [24/50] batch [15/25] time 41.432 (15.931) data 0.003 (0.090) loss 0.5712 (0.8891) acc 87.5000 (72.9167) lr 1.1874e-03 eta 2:55:14
epoch [24/50] batch [20/25] time 27.224 (16.378) data 0.005 (0.069) loss 0.9723 (0.9164) acc 65.6250 (71.7188) lr 1.1874e-03 eta 2:58:47
epoch [24/50] batch [25/25] time 13.250 (16.971) data 0.006 (0.057) loss 0.8801 (0.9232) acc 71.8750 (71.5000) lr 1.1253e-03 eta 3:03:50
epoch [25/50] batch [5/25] time 13.163 (13.806) data 0.007 (0.243) loss 1.0599 (1.0373) acc 71.8750 (68.1250) lr 1.1253e-03 eta 2:28:25
epoch [25/50] batch [10/25] time 12.951 (15.227) data 0.007 (0.125) loss 0.8839 (0.9666) acc 75.0000 (71.2500) lr 1.1253e-03 eta 2:42:25
epoch [25/50] batch [15/25] time 12.952 (14.503) data 0.004 (0.085) loss 0.9294 (0.9910) acc 71.8750 (70.4167) lr 1.1253e-03 eta 2:33:29
epoch [25/50] batch [20/25] time 27.585 (14.856) data 0.005 (0.065) loss 0.7657 (0.9776) acc 75.0000 (71.2500) lr 1.1253e-03 eta 2:35:58
epoch [25/50] batch [25/25] time 13.309 (14.447) data 0.005 (0.053) loss 0.9616 (0.9723) acc 71.8750 (71.2500) lr 1.0628e-03 eta 2:30:29
epoch [26/50] batch [5/25] time 13.138 (13.756) data 0.006 (0.253) loss 1.0798 (1.0129) acc 75.0000 (70.6250) lr 1.0628e-03 eta 2:22:08
epoch [26/50] batch [10/25] time 13.101 (16.569) data 0.003 (0.144) loss 0.8299 (1.0468) acc 68.7500 (65.3125) lr 1.0628e-03 eta 2:49:50
epoch [26/50] batch [15/25] time 12.995 (15.385) data 0.004 (0.098) loss 0.6042 (0.9566) acc 84.3750 (69.1667) lr 1.0628e-03 eta 2:36:24
epoch [26/50] batch [20/25] time 27.453 (15.547) data 0.006 (0.075) loss 0.6643 (0.9474) acc 84.3750 (69.6875) lr 1.0628e-03 eta 2:36:45
epoch [26/50] batch [25/25] time 12.406 (16.100) data 0.005 (0.061) loss 0.8984 (0.9418) acc 71.8750 (70.1250) lr 1.0000e-03 eta 2:41:00
epoch [27/50] batch [5/25] time 17.159 (16.362) data 0.004 (0.262) loss 1.1326 (0.9375) acc 68.7500 (72.5000) lr 1.0000e-03 eta 2:42:15
epoch [27/50] batch [10/25] time 23.790 (19.535) data 0.006 (0.137) loss 0.8286 (0.9533) acc 68.7500 (69.0625) lr 1.0000e-03 eta 3:12:05
epoch [27/50] batch [15/25] time 23.361 (19.247) data 0.007 (0.094) loss 0.7804 (0.8950) acc 71.8750 (70.2083) lr 1.0000e-03 eta 3:07:39
epoch [27/50] batch [20/25] time 13.139 (17.639) data 0.006 (0.072) loss 1.0380 (0.8812) acc 65.6250 (71.2500) lr 1.0000e-03 eta 2:50:30
epoch [27/50] batch [25/25] time 12.671 (18.658) data 0.005 (0.059) loss 0.7765 (0.8816) acc 71.8750 (71.7500) lr 9.3721e-04 eta 2:58:48
epoch [28/50] batch [5/25] time 12.536 (20.463) data 0.006 (0.225) loss 1.0300 (0.7701) acc 68.7500 (77.5000) lr 9.3721e-04 eta 3:14:23
epoch [28/50] batch [10/25] time 27.923 (19.572) data 0.004 (0.119) loss 0.7253 (0.8683) acc 81.2500 (75.3125) lr 9.3721e-04 eta 3:04:18
epoch [28/50] batch [15/25] time 13.101 (18.231) data 0.004 (0.082) loss 0.9072 (0.8945) acc 71.8750 (73.7500) lr 9.3721e-04 eta 2:50:09
epoch [28/50] batch [20/25] time 13.137 (16.950) data 0.006 (0.063) loss 0.6133 (0.8980) acc 81.2500 (72.8125) lr 9.3721e-04 eta 2:36:47
epoch [28/50] batch [25/25] time 13.000 (16.167) data 0.006 (0.051) loss 0.7466 (0.8525) acc 84.3750 (74.3750) lr 8.7467e-04 eta 2:28:11
epoch [29/50] batch [5/25] time 13.172 (13.806) data 0.004 (0.223) loss 0.4795 (0.8057) acc 87.5000 (75.6250) lr 8.7467e-04 eta 2:05:24
epoch [29/50] batch [10/25] time 12.865 (18.302) data 0.005 (0.118) loss 0.6247 (0.8283) acc 81.2500 (74.0625) lr 8.7467e-04 eta 2:44:43
epoch [29/50] batch [15/25] time 12.882 (17.857) data 0.023 (0.082) loss 1.6159 (0.8835) acc 43.7500 (72.2917) lr 8.7467e-04 eta 2:39:13
epoch [29/50] batch [20/25] time 14.905 (16.757) data 0.006 (0.063) loss 0.7724 (0.8945) acc 78.1250 (72.6562) lr 8.7467e-04 eta 2:28:01
epoch [29/50] batch [25/25] time 12.899 (17.948) data 0.005 (0.053) loss 0.6189 (0.8451) acc 81.2500 (74.0000) lr 8.1262e-04 eta 2:37:02
epoch [30/50] batch [5/25] time 17.351 (24.869) data 0.020 (0.451) loss 0.8258 (0.8631) acc 75.0000 (74.3750) lr 8.1262e-04 eta 3:35:31
epoch [30/50] batch [10/25] time 12.561 (20.210) data 0.003 (0.230) loss 0.9515 (0.8710) acc 71.8750 (74.6875) lr 8.1262e-04 eta 2:53:28
epoch [30/50] batch [15/25] time 12.852 (20.030) data 0.004 (0.155) loss 1.3851 (0.9057) acc 59.3750 (72.9167) lr 8.1262e-04 eta 2:50:15
epoch [30/50] batch [20/25] time 31.330 (19.726) data 0.008 (0.119) loss 1.0677 (0.8821) acc 65.6250 (73.5938) lr 8.1262e-04 eta 2:46:01
epoch [30/50] batch [25/25] time 13.034 (18.328) data 0.007 (0.096) loss 0.7674 (0.8758) acc 71.8750 (73.6250) lr 7.5131e-04 eta 2:32:44
epoch [31/50] batch [5/25] time 49.961 (26.351) data 0.012 (0.227) loss 0.9174 (0.7777) acc 62.5000 (74.3750) lr 7.5131e-04 eta 3:37:23
epoch [31/50] batch [10/25] time 13.004 (19.592) data 0.009 (0.117) loss 0.9397 (0.8348) acc 68.7500 (73.4375) lr 7.5131e-04 eta 2:40:00
epoch [31/50] batch [15/25] time 13.018 (17.392) data 0.004 (0.080) loss 0.7057 (0.8471) acc 78.1250 (74.1667) lr 7.5131e-04 eta 2:20:35
epoch [31/50] batch [20/25] time 12.937 (16.295) data 0.006 (0.061) loss 0.6501 (0.8411) acc 75.0000 (74.3750) lr 7.5131e-04 eta 2:10:21
epoch [31/50] batch [25/25] time 12.985 (15.631) data 0.005 (0.050) loss 1.2661 (0.8474) acc 50.0000 (73.0000) lr 6.9098e-04 eta 2:03:44
epoch [32/50] batch [5/25] time 13.223 (17.865) data 0.004 (0.204) loss 0.9501 (0.8132) acc 71.8750 (74.3750) lr 6.9098e-04 eta 2:19:56
epoch [32/50] batch [10/25] time 23.359 (19.860) data 0.007 (0.105) loss 0.7710 (0.8523) acc 68.7500 (73.1250) lr 6.9098e-04 eta 2:33:54
epoch [32/50] batch [15/25] time 42.296 (20.670) data 0.004 (0.072) loss 1.1310 (0.8555) acc 71.8750 (73.3333) lr 6.9098e-04 eta 2:38:28
epoch [32/50] batch [20/25] time 13.135 (18.663) data 0.005 (0.056) loss 1.2697 (0.8847) acc 62.5000 (72.9688) lr 6.9098e-04 eta 2:21:31
epoch [32/50] batch [25/25] time 27.216 (19.346) data 0.005 (0.046) loss 0.7360 (0.8826) acc 84.3750 (73.6250) lr 6.3188e-04 eta 2:25:05
epoch [33/50] batch [5/25] time 13.213 (18.108) data 0.007 (0.509) loss 0.7390 (0.7950) acc 68.7500 (74.3750) lr 6.3188e-04 eta 2:14:18
epoch [33/50] batch [10/25] time 11.922 (21.904) data 0.014 (0.260) loss 0.8483 (0.8275) acc 75.0000 (73.4375) lr 6.3188e-04 eta 2:40:37
epoch [33/50] batch [15/25] time 13.023 (18.874) data 0.006 (0.175) loss 0.6740 (0.8405) acc 75.0000 (73.3333) lr 6.3188e-04 eta 2:16:50
epoch [33/50] batch [20/25] time 25.183 (18.037) data 0.006 (0.133) loss 0.8378 (0.8367) acc 78.1250 (74.2188) lr 6.3188e-04 eta 2:09:16
epoch [33/50] batch [25/25] time 11.840 (18.199) data 0.005 (0.109) loss 1.0295 (0.8601) acc 62.5000 (73.2500) lr 5.7422e-04 eta 2:08:54
epoch [34/50] batch [5/25] time 13.180 (14.090) data 0.004 (0.313) loss 0.8688 (0.9291) acc 68.7500 (70.6250) lr 5.7422e-04 eta 1:38:37
epoch [34/50] batch [10/25] time 13.182 (13.639) data 0.004 (0.160) loss 1.2081 (0.8509) acc 65.6250 (73.7500) lr 5.7422e-04 eta 1:34:20
epoch [34/50] batch [15/25] time 12.810 (16.674) data 0.005 (0.110) loss 0.6430 (0.8514) acc 84.3750 (73.3333) lr 5.7422e-04 eta 1:53:56
epoch [34/50] batch [20/25] time 12.612 (18.017) data 0.005 (0.084) loss 0.9375 (0.8580) acc 71.8750 (73.2812) lr 5.7422e-04 eta 2:01:37
epoch [34/50] batch [25/25] time 13.019 (17.017) data 0.006 (0.069) loss 0.9921 (0.8627) acc 68.7500 (73.3750) lr 5.1825e-04 eta 1:53:26
epoch [35/50] batch [5/25] time 12.929 (23.594) data 0.007 (0.291) loss 0.9579 (0.8825) acc 68.7500 (72.5000) lr 5.1825e-04 eta 2:35:19
epoch [35/50] batch [10/25] time 12.516 (22.032) data 0.009 (0.149) loss 0.4922 (0.8458) acc 84.3750 (73.7500) lr 5.1825e-04 eta 2:23:12
epoch [35/50] batch [15/25] time 39.606 (21.692) data 0.004 (0.101) loss 0.9383 (0.8546) acc 84.3750 (75.4167) lr 5.1825e-04 eta 2:19:11
epoch [35/50] batch [20/25] time 13.004 (20.433) data 0.006 (0.078) loss 1.0496 (0.8638) acc 68.7500 (74.5312) lr 5.1825e-04 eta 2:09:24
epoch [35/50] batch [25/25] time 13.002 (18.947) data 0.006 (0.064) loss 1.2030 (0.8550) acc 68.7500 (75.0000) lr 4.6417e-04 eta 1:58:25
epoch [36/50] batch [5/25] time 13.251 (13.803) data 0.004 (0.257) loss 0.7623 (1.0222) acc 81.2500 (69.3750) lr 4.6417e-04 eta 1:25:07
epoch [36/50] batch [10/25] time 13.044 (13.493) data 0.004 (0.131) loss 1.1371 (0.9747) acc 65.6250 (71.5625) lr 4.6417e-04 eta 1:22:05
epoch [36/50] batch [15/25] time 12.992 (13.329) data 0.004 (0.089) loss 0.6761 (0.9444) acc 84.3750 (72.5000) lr 4.6417e-04 eta 1:19:58
epoch [36/50] batch [20/25] time 12.975 (13.251) data 0.006 (0.068) loss 0.5367 (0.8736) acc 84.3750 (74.6875) lr 4.6417e-04 eta 1:18:23
epoch [36/50] batch [25/25] time 13.019 (13.208) data 0.005 (0.056) loss 1.1000 (0.9020) acc 68.7500 (74.1250) lr 4.1221e-04 eta 1:17:02
epoch [37/50] batch [5/25] time 13.212 (13.845) data 0.004 (0.263) loss 0.9481 (0.9110) acc 68.7500 (73.7500) lr 4.1221e-04 eta 1:19:36
epoch [37/50] batch [10/25] time 12.948 (13.473) data 0.004 (0.135) loss 1.1092 (0.8883) acc 68.7500 (73.1250) lr 4.1221e-04 eta 1:16:20
epoch [37/50] batch [15/25] time 12.974 (13.316) data 0.006 (0.091) loss 0.8931 (0.9403) acc 75.0000 (72.5000) lr 4.1221e-04 eta 1:14:20
epoch [37/50] batch [20/25] time 12.990 (13.259) data 0.006 (0.070) loss 0.4498 (0.9026) acc 90.6250 (73.2812) lr 4.1221e-04 eta 1:12:55
epoch [37/50] batch [25/25] time 13.002 (13.221) data 0.006 (0.057) loss 0.8224 (0.8809) acc 81.2500 (74.1250) lr 3.6258e-04 eta 1:11:36
epoch [38/50] batch [5/25] time 12.489 (20.048) data 0.004 (0.213) loss 1.1511 (0.9012) acc 65.6250 (75.6250) lr 3.6258e-04 eta 1:46:55
epoch [38/50] batch [10/25] time 12.666 (18.050) data 0.012 (0.112) loss 0.9816 (0.8995) acc 62.5000 (73.1250) lr 3.6258e-04 eta 1:34:45
epoch [38/50] batch [15/25] time 12.973 (18.460) data 0.004 (0.076) loss 0.6425 (0.8980) acc 71.8750 (72.9167) lr 3.6258e-04 eta 1:35:22
epoch [38/50] batch [20/25] time 18.469 (18.659) data 0.006 (0.059) loss 0.6600 (0.9033) acc 78.1250 (72.9688) lr 3.6258e-04 eta 1:34:50
epoch [38/50] batch [25/25] time 13.173 (18.205) data 0.006 (0.048) loss 0.8555 (0.8650) acc 68.7500 (73.6250) lr 3.1545e-04 eta 1:31:01
epoch [39/50] batch [5/25] time 13.080 (20.623) data 0.003 (0.257) loss 0.6876 (0.7055) acc 81.2500 (77.5000) lr 3.1545e-04 eta 1:41:23
epoch [39/50] batch [10/25] time 12.813 (18.209) data 0.005 (0.131) loss 0.8151 (0.7358) acc 68.7500 (76.8750) lr 3.1545e-04 eta 1:28:00
epoch [39/50] batch [15/25] time 13.162 (18.252) data 0.004 (0.090) loss 1.1299 (0.8270) acc 71.8750 (73.9583) lr 3.1545e-04 eta 1:26:41
epoch [39/50] batch [20/25] time 12.944 (16.944) data 0.005 (0.069) loss 0.8151 (0.8256) acc 81.2500 (74.5312) lr 3.1545e-04 eta 1:19:04
epoch [39/50] batch [25/25] time 27.631 (16.752) data 0.006 (0.056) loss 0.9896 (0.8465) acc 68.7500 (74.7500) lr 2.7103e-04 eta 1:16:46
epoch [40/50] batch [5/25] time 47.008 (26.118) data 0.015 (3.731) loss 1.2313 (0.9230) acc 62.5000 (75.0000) lr 2.7103e-04 eta 1:57:31
epoch [40/50] batch [10/25] time 13.115 (19.770) data 0.004 (1.870) loss 0.7344 (0.8789) acc 78.1250 (75.0000) lr 2.7103e-04 eta 1:27:19
epoch [40/50] batch [15/25] time 13.046 (17.528) data 0.004 (1.248) loss 0.7033 (0.8979) acc 78.1250 (72.5000) lr 2.7103e-04 eta 1:15:57
epoch [40/50] batch [20/25] time 12.671 (18.665) data 0.005 (0.938) loss 0.6771 (0.8721) acc 81.2500 (73.7500) lr 2.7103e-04 eta 1:19:19
epoch [40/50] batch [25/25] time 12.818 (18.619) data 0.005 (0.752) loss 0.7749 (0.8291) acc 81.2500 (74.7500) lr 2.2949e-04 eta 1:17:34
epoch [41/50] batch [5/25] time 12.739 (26.405) data 0.009 (0.229) loss 0.9992 (0.7717) acc 65.6250 (75.6250) lr 2.2949e-04 eta 1:47:49
epoch [41/50] batch [10/25] time 11.892 (28.383) data 0.003 (0.118) loss 0.7628 (0.7751) acc 81.2500 (76.8750) lr 2.2949e-04 eta 1:53:31
epoch [41/50] batch [15/25] time 12.948 (23.210) data 0.004 (0.080) loss 0.9781 (0.8149) acc 75.0000 (75.0000) lr 2.2949e-04 eta 1:30:54
epoch [41/50] batch [20/25] time 13.410 (22.318) data 0.005 (0.063) loss 0.4555 (0.8161) acc 93.7500 (75.3125) lr 2.2949e-04 eta 1:25:33
epoch [41/50] batch [25/25] time 13.002 (20.470) data 0.006 (0.051) loss 0.9692 (0.8281) acc 71.8750 (75.0000) lr 1.9098e-04 eta 1:16:45
epoch [42/50] batch [5/25] time 13.082 (13.708) data 0.004 (0.263) loss 0.7654 (0.8040) acc 75.0000 (74.3750) lr 1.9098e-04 eta 0:50:15
epoch [42/50] batch [10/25] time 46.597 (20.407) data 0.015 (0.136) loss 0.5516 (0.7721) acc 78.1250 (77.1875) lr 1.9098e-04 eta 1:13:07
epoch [42/50] batch [15/25] time 13.044 (18.032) data 0.004 (0.093) loss 1.1929 (0.8556) acc 65.6250 (74.5833) lr 1.9098e-04 eta 1:03:06
epoch [42/50] batch [20/25] time 12.970 (17.694) data 0.006 (0.072) loss 0.7315 (0.8420) acc 75.0000 (75.1562) lr 1.9098e-04 eta 1:00:27
epoch [42/50] batch [25/25] time 12.955 (16.746) data 0.006 (0.059) loss 1.0081 (0.8285) acc 59.3750 (74.6250) lr 1.5567e-04 eta 0:55:49
epoch [43/50] batch [5/25] time 50.188 (25.957) data 0.003 (0.181) loss 0.6675 (0.7425) acc 71.8750 (72.5000) lr 1.5567e-04 eta 1:24:21
epoch [43/50] batch [10/25] time 46.239 (24.062) data 0.033 (0.096) loss 0.7194 (0.7829) acc 75.0000 (73.4375) lr 1.5567e-04 eta 1:16:11
epoch [43/50] batch [15/25] time 12.978 (20.437) data 0.004 (0.067) loss 0.8006 (0.8133) acc 81.2500 (73.9583) lr 1.5567e-04 eta 1:03:00
epoch [43/50] batch [20/25] time 27.946 (20.002) data 0.006 (0.052) loss 0.9642 (0.8463) acc 78.1250 (73.9062) lr 1.5567e-04 eta 1:00:00
epoch [43/50] batch [25/25] time 24.777 (20.453) data 0.005 (0.044) loss 1.2571 (0.8466) acc 62.5000 (73.8750) lr 1.2369e-04 eta 0:59:39
epoch [44/50] batch [5/25] time 13.154 (16.501) data 0.004 (3.010) loss 0.6034 (0.8274) acc 90.6250 (73.1250) lr 1.2369e-04 eta 0:46:45
epoch [44/50] batch [10/25] time 12.963 (16.117) data 0.004 (1.508) loss 1.2117 (0.8574) acc 62.5000 (72.1875) lr 1.2369e-04 eta 0:44:19
epoch [44/50] batch [15/25] time 49.822 (19.110) data 0.003 (1.007) loss 0.7906 (0.8313) acc 81.2500 (74.1667) lr 1.2369e-04 eta 0:50:57
epoch [44/50] batch [20/25] time 12.743 (20.182) data 0.005 (0.757) loss 0.7403 (0.8494) acc 75.0000 (74.8438) lr 1.2369e-04 eta 0:52:08
epoch [44/50] batch [25/25] time 12.928 (18.734) data 0.007 (0.607) loss 0.6824 (0.8665) acc 81.2500 (74.3750) lr 9.5173e-05 eta 0:46:50
epoch [45/50] batch [5/25] time 12.946 (23.984) data 0.004 (0.215) loss 0.6720 (0.7831) acc 78.1250 (75.0000) lr 9.5173e-05 eta 0:57:57
epoch [45/50] batch [10/25] time 12.666 (21.250) data 0.010 (0.112) loss 0.9791 (0.8324) acc 81.2500 (74.6875) lr 9.5173e-05 eta 0:49:35
epoch [45/50] batch [15/25] time 12.954 (18.487) data 0.006 (0.076) loss 0.5979 (0.8320) acc 84.3750 (75.6250) lr 9.5173e-05 eta 0:41:35
epoch [45/50] batch [20/25] time 13.188 (17.125) data 0.006 (0.059) loss 0.8423 (0.8320) acc 71.8750 (74.8438) lr 9.5173e-05 eta 0:37:06
epoch [45/50] batch [25/25] time 12.769 (17.766) data 0.005 (0.049) loss 1.2439 (0.8364) acc 71.8750 (75.7500) lr 7.0224e-05 eta 0:37:00
epoch [46/50] batch [5/25] time 22.458 (23.957) data 0.004 (0.362) loss 0.8179 (0.8919) acc 71.8750 (71.8750) lr 7.0224e-05 eta 0:47:54
epoch [46/50] batch [10/25] time 12.994 (21.377) data 0.003 (0.186) loss 0.8507 (0.7842) acc 71.8750 (76.2500) lr 7.0224e-05 eta 0:40:58
epoch [46/50] batch [15/25] time 12.947 (18.588) data 0.004 (0.126) loss 0.8428 (0.8274) acc 68.7500 (73.7500) lr 7.0224e-05 eta 0:34:04
epoch [46/50] batch [20/25] time 36.561 (18.375) data 0.005 (0.096) loss 1.1379 (0.8144) acc 65.6250 (74.3750) lr 7.0224e-05 eta 0:32:09
epoch [46/50] batch [25/25] time 12.954 (17.297) data 0.006 (0.078) loss 1.0768 (0.8004) acc 68.7500 (74.8750) lr 4.8943e-05 eta 0:28:49
epoch [47/50] batch [5/25] time 12.794 (27.717) data 0.004 (0.252) loss 0.7429 (0.8242) acc 81.2500 (73.1250) lr 4.8943e-05 eta 0:43:53
epoch [47/50] batch [10/25] time 13.077 (20.435) data 0.004 (0.130) loss 0.8185 (0.7947) acc 65.6250 (74.6875) lr 4.8943e-05 eta 0:30:39
epoch [47/50] batch [15/25] time 13.040 (17.972) data 0.004 (0.088) loss 1.0984 (0.8408) acc 65.6250 (73.1250) lr 4.8943e-05 eta 0:25:27
epoch [47/50] batch [20/25] time 12.963 (16.742) data 0.006 (0.068) loss 0.6228 (0.8010) acc 84.3750 (74.3750) lr 4.8943e-05 eta 0:22:19
epoch [47/50] batch [25/25] time 12.654 (16.633) data 0.005 (0.055) loss 0.9821 (0.7909) acc 59.3750 (75.3750) lr 3.1417e-05 eta 0:20:47
epoch [48/50] batch [5/25] time 13.130 (13.765) data 0.004 (0.246) loss 0.8804 (1.0205) acc 81.2500 (70.6250) lr 3.1417e-05 eta 0:16:03
epoch [48/50] batch [10/25] time 12.618 (18.269) data 0.005 (0.128) loss 1.1739 (0.9242) acc 65.6250 (72.5000) lr 3.1417e-05 eta 0:19:47
epoch [48/50] batch [15/25] time 16.207 (17.638) data 0.004 (0.088) loss 0.7968 (0.8923) acc 68.7500 (72.9167) lr 3.1417e-05 eta 0:17:38
epoch [48/50] batch [20/25] time 12.962 (17.764) data 0.006 (0.071) loss 0.9051 (0.8693) acc 75.0000 (73.5938) lr 3.1417e-05 eta 0:16:17
epoch [48/50] batch [25/25] time 12.971 (16.804) data 0.006 (0.058) loss 0.7623 (0.8450) acc 68.7500 (73.7500) lr 1.7713e-05 eta 0:14:00
epoch [49/50] batch [5/25] time 13.164 (16.746) data 0.007 (0.239) loss 0.7651 (0.8427) acc 68.7500 (71.8750) lr 1.7713e-05 eta 0:12:33
epoch [49/50] batch [10/25] time 13.018 (14.942) data 0.006 (0.122) loss 0.9996 (0.8070) acc 65.6250 (72.8125) lr 1.7713e-05 eta 0:09:57
epoch [49/50] batch [15/25] time 12.970 (14.296) data 0.004 (0.084) loss 1.2864 (0.8393) acc 68.7500 (72.5000) lr 1.7713e-05 eta 0:08:20
epoch [49/50] batch [20/25] time 12.968 (13.968) data 0.005 (0.064) loss 0.6885 (0.8387) acc 75.0000 (72.6562) lr 1.7713e-05 eta 0:06:59
epoch [49/50] batch [25/25] time 12.968 (13.770) data 0.006 (0.053) loss 1.0869 (0.8655) acc 62.5000 (71.8750) lr 7.8853e-06 eta 0:05:44
epoch [50/50] batch [5/25] time 13.358 (13.816) data 0.008 (0.260) loss 1.1013 (0.8784) acc 59.3750 (71.2500) lr 7.8853e-06 eta 0:04:36
epoch [50/50] batch [10/25] time 12.999 (13.505) data 0.009 (0.133) loss 0.6681 (0.8065) acc 84.3750 (75.3125) lr 7.8853e-06 eta 0:03:22
epoch [50/50] batch [15/25] time 12.918 (13.320) data 0.004 (0.090) loss 0.8067 (0.7714) acc 71.8750 (76.2500) lr 7.8853e-06 eta 0:02:13
epoch [50/50] batch [20/25] time 13.035 (13.235) data 0.006 (0.069) loss 0.8060 (0.7973) acc 65.6250 (75.6250) lr 7.8853e-06 eta 0:01:06
epoch [50/50] batch [25/25] time 13.122 (13.196) data 0.006 (0.057) loss 0.6724 (0.7939) acc 84.3750 (76.2500) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16_8.0/ResidualPrompting/vit_b16_ep50_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
=> result
* total: 1,053
* correct: 865
* accuracy: 82.1%
* error: 17.9%
* macro_f1: 80.2%
Elapsed: 5:55:19
