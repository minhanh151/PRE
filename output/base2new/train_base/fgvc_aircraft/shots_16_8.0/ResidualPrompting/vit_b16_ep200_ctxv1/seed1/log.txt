***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/ResidualPrompting/vit_b16_ep200_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '8.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output/base2new/train_base/fgvc_aircraft/shots_16_8.0/ResidualPrompting/vit_b16_ep200_ctxv1/seed1
resume: 
root: /home/ducan/Downloads/MinhAnh/CoOp/DATA
seed: 1
source_domains: None
target_domains: None
trainer: ResidualPrompting
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 100
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /home/ducan/Downloads/MinhAnh/CoOp/DATA
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 200
  MOMENTUM: 0.9
  NAME: sgd
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output/base2new/train_base/fgvc_aircraft/shots_16_8.0/ResidualPrompting/vit_b16_ep200_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: 
    N_CTX: 16
    PREC: amp
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: a photo of a
    N_CTX: 4
    PREC: amp
    W: 8.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: ResidualPrompting
  PLOT:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N: 4
    N_CTX: 16
    PREC: amp
  ResidualPrompting:
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: 
    N_CTX: 16
    PREC: amp
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.10.2
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.16.3
Libc version: glibc-2.31

Python version: 3.8.16 (default, Jun 12 2023, 18:09:05)  [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.17
Is CUDA available: False
CUDA runtime version: 12.2.91
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.24.2
[pip3] torch==1.10.2
[pip3] torchvision==0.11.3
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2023.1.0         h6d00ec8_46342  
[conda] mkl-service               2.4.0            py38h5eee18b_1  
[conda] mkl_fft                   1.3.6            py38h417a72b_1  
[conda] mkl_random                1.2.2            py38h417a72b_1  
[conda] numpy                     1.24.3           py38hf6e8229_1  
[conda] numpy-base                1.24.3           py38h060ed82_1  
[conda] pytorch                   1.10.2          py3.8_cuda10.2_cudnn7.6.5_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchvision               0.11.3               py38_cu102    pytorch
        Pillow (9.4.0)

Loading trainer: ResidualPrompting
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /home/ducan/Downloads/MinhAnh/CoOp/DATA/fgvc_aircraft/data/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,666
---------  ------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X X X X X X X X X X X X X"
Number of context words (tokens): 16
Using skip connection in MLP
Turning off gradients in both the image and the text encoder
Loading evaluator: Classification
output/base2new/train_base/fgvc_aircraft/shots_16_8.0/ResidualPrompting/vit_b16_ep200_ctxv1/seed1
['prompt_learner']
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output/base2new/train_base/fgvc_aircraft/shots_16_8.0/ResidualPrompting/vit_b16_ep200_ctxv1/seed1/tensorboard)
epoch [1/200] batch [5/25] time 21.260 (22.228) data 0.005 (0.855) loss 5.7037 (5.3749) acc 0.0000 (6.2500) lr 1.0000e-05 eta 1 day, 6:50:28
epoch [1/200] batch [10/25] time 18.031 (20.068) data 0.006 (0.432) loss 5.1791 (5.2840) acc 3.1250 (6.2500) lr 1.0000e-05 eta 1 day, 3:48:58
epoch [1/200] batch [15/25] time 23.541 (20.435) data 0.005 (0.290) loss 4.8542 (5.2407) acc 0.0000 (5.6250) lr 1.0000e-05 eta 1 day, 4:17:47
epoch [1/200] batch [20/25] time 19.250 (21.101) data 0.006 (0.219) loss 4.6448 (5.1078) acc 9.3750 (6.0938) lr 1.0000e-05 eta 1 day, 5:11:22
epoch [1/200] batch [25/25] time 21.121 (20.510) data 0.008 (0.177) loss 4.5534 (5.0098) acc 6.2500 (6.7500) lr 2.0000e-03 eta 1 day, 4:20:35
epoch [2/200] batch [5/25] time 21.517 (19.513) data 0.004 (0.770) loss 2.6871 (3.0320) acc 28.1250 (17.5000) lr 2.0000e-03 eta 1 day, 2:56:21
epoch [2/200] batch [10/25] time 19.390 (19.671) data 0.005 (0.390) loss 2.6721 (2.8955) acc 37.5000 (22.8125) lr 2.0000e-03 eta 1 day, 3:07:46
epoch [2/200] batch [15/25] time 70.888 (27.921) data 0.007 (0.264) loss 2.8526 (2.8777) acc 28.1250 (21.8750) lr 2.0000e-03 eta 1 day, 14:28:07
epoch [2/200] batch [20/25] time 32.907 (26.352) data 0.008 (0.200) loss 2.4120 (2.8491) acc 25.0000 (20.7812) lr 2.0000e-03 eta 1 day, 12:16:14
epoch [2/200] batch [25/25] time 23.701 (24.792) data 0.007 (0.162) loss 2.3889 (2.7841) acc 34.3750 (22.0000) lr 1.9999e-03 eta 1 day, 10:05:18
epoch [3/200] batch [5/25] time 13.362 (27.605) data 0.004 (0.910) loss 2.1511 (2.5382) acc 34.3750 (26.8750) lr 1.9999e-03 eta 1 day, 13:55:07
epoch [3/200] batch [10/25] time 13.060 (20.453) data 0.004 (0.459) loss 2.5404 (2.5121) acc 21.8750 (25.6250) lr 1.9999e-03 eta 1 day, 4:03:55
epoch [3/200] batch [15/25] time 12.892 (18.031) data 0.004 (0.307) loss 2.4521 (2.4432) acc 18.7500 (26.0417) lr 1.9999e-03 eta 1 day, 0:43:04
epoch [3/200] batch [20/25] time 13.332 (16.817) data 0.005 (0.232) loss 2.6454 (2.4598) acc 28.1250 (25.0000) lr 1.9999e-03 eta 23:01:49
epoch [3/200] batch [25/25] time 13.044 (16.064) data 0.006 (0.187) loss 2.6785 (2.4764) acc 15.6250 (24.8750) lr 1.9995e-03 eta 21:58:35
epoch [4/200] batch [5/25] time 13.227 (14.631) data 0.004 (0.780) loss 2.5798 (2.5418) acc 21.8750 (23.7500) lr 1.9995e-03 eta 19:59:46
epoch [4/200] batch [10/25] time 12.901 (13.935) data 0.004 (0.394) loss 1.9341 (2.4110) acc 34.3750 (25.0000) lr 1.9995e-03 eta 19:01:32
epoch [4/200] batch [15/25] time 13.230 (13.631) data 0.072 (0.269) loss 2.3600 (2.3987) acc 25.0000 (25.4167) lr 1.9995e-03 eta 18:35:29
epoch [4/200] batch [20/25] time 13.213 (13.530) data 0.006 (0.203) loss 2.6318 (2.4466) acc 18.7500 (24.6875) lr 1.9995e-03 eta 18:26:04
epoch [4/200] batch [25/25] time 13.050 (13.468) data 0.007 (0.164) loss 2.4927 (2.4292) acc 21.8750 (24.7500) lr 1.9989e-03 eta 18:19:51
epoch [5/200] batch [5/25] time 13.618 (14.513) data 0.004 (0.551) loss 2.4728 (2.4299) acc 25.0000 (21.8750) lr 1.9989e-03 eta 19:43:59
epoch [5/200] batch [10/25] time 13.221 (13.988) data 0.004 (0.279) loss 2.3518 (2.4283) acc 28.1250 (22.8125) lr 1.9989e-03 eta 19:00:02
epoch [5/200] batch [15/25] time 13.324 (13.730) data 0.004 (0.187) loss 2.2685 (2.4262) acc 37.5000 (23.7500) lr 1.9989e-03 eta 18:37:51
epoch [5/200] batch [20/25] time 13.271 (13.610) data 0.006 (0.142) loss 2.6172 (2.4067) acc 15.6250 (22.9688) lr 1.9989e-03 eta 18:26:57
epoch [5/200] batch [25/25] time 12.940 (13.518) data 0.006 (0.115) loss 2.4324 (2.3712) acc 21.8750 (24.5000) lr 1.9980e-03 eta 18:18:19
epoch [6/200] batch [5/25] time 13.369 (14.398) data 0.004 (0.510) loss 2.6861 (2.5385) acc 18.7500 (21.8750) lr 1.9980e-03 eta 19:28:39
epoch [6/200] batch [10/25] time 12.939 (13.825) data 0.006 (0.258) loss 2.3161 (2.3720) acc 25.0000 (26.8750) lr 1.9980e-03 eta 18:40:58
epoch [6/200] batch [15/25] time 13.201 (13.568) data 0.004 (0.174) loss 2.3588 (2.3679) acc 25.0000 (27.0833) lr 1.9980e-03 eta 18:19:00
epoch [6/200] batch [20/25] time 12.903 (13.421) data 0.006 (0.132) loss 2.2747 (2.3367) acc 21.8750 (27.1875) lr 1.9980e-03 eta 18:05:58
epoch [6/200] batch [25/25] time 12.926 (13.345) data 0.006 (0.107) loss 2.4763 (2.3370) acc 28.1250 (26.6250) lr 1.9969e-03 eta 17:58:40
epoch [7/200] batch [5/25] time 13.340 (14.509) data 0.010 (0.539) loss 2.4362 (2.3358) acc 21.8750 (30.0000) lr 1.9969e-03 eta 19:31:35
epoch [7/200] batch [10/25] time 13.241 (14.083) data 0.006 (0.273) loss 2.6079 (2.3903) acc 25.0000 (27.5000) lr 1.9969e-03 eta 18:56:02
epoch [7/200] batch [15/25] time 13.234 (13.778) data 0.004 (0.184) loss 2.3623 (2.3909) acc 18.7500 (28.3333) lr 1.9969e-03 eta 18:30:17
epoch [7/200] batch [20/25] time 12.929 (13.595) data 0.006 (0.139) loss 2.4410 (2.3790) acc 31.2500 (28.1250) lr 1.9969e-03 eta 18:14:23
epoch [7/200] batch [25/25] time 13.076 (13.495) data 0.006 (0.113) loss 2.1370 (2.3659) acc 25.0000 (27.3750) lr 1.9956e-03 eta 18:05:15
epoch [8/200] batch [5/25] time 13.305 (14.633) data 0.004 (0.548) loss 2.4232 (2.4013) acc 31.2500 (27.5000) lr 1.9956e-03 eta 19:35:31
epoch [8/200] batch [10/25] time 12.931 (13.976) data 0.004 (0.277) loss 1.9939 (2.3142) acc 37.5000 (30.6250) lr 1.9956e-03 eta 18:41:32
epoch [8/200] batch [15/25] time 13.205 (13.687) data 0.004 (0.191) loss 1.8834 (2.2927) acc 43.7500 (30.8333) lr 1.9956e-03 eta 18:17:13
epoch [8/200] batch [20/25] time 12.920 (13.504) data 0.006 (0.144) loss 2.0114 (2.3068) acc 37.5000 (29.8438) lr 1.9956e-03 eta 18:01:27
epoch [8/200] batch [25/25] time 13.123 (13.399) data 0.006 (0.117) loss 2.2913 (2.3060) acc 25.0000 (29.5000) lr 1.9940e-03 eta 17:51:55
epoch [9/200] batch [5/25] time 13.368 (14.541) data 0.004 (0.558) loss 2.1233 (2.2102) acc 37.5000 (31.8750) lr 1.9940e-03 eta 19:22:05
epoch [9/200] batch [10/25] time 12.966 (13.927) data 0.012 (0.283) loss 2.4140 (2.2795) acc 37.5000 (30.6250) lr 1.9940e-03 eta 18:31:52
epoch [9/200] batch [15/25] time 12.964 (13.603) data 0.004 (0.190) loss 2.4475 (2.2788) acc 21.8750 (28.1250) lr 1.9940e-03 eta 18:04:49
epoch [9/200] batch [20/25] time 12.917 (13.443) data 0.006 (0.144) loss 1.9093 (2.2834) acc 40.6250 (28.1250) lr 1.9940e-03 eta 17:50:55
epoch [9/200] batch [25/25] time 12.935 (13.367) data 0.006 (0.116) loss 2.3317 (2.3027) acc 21.8750 (27.7500) lr 1.9921e-03 eta 17:43:47
epoch [10/200] batch [5/25] time 13.334 (14.614) data 0.010 (0.460) loss 2.4242 (2.2878) acc 25.0000 (32.5000) lr 1.9921e-03 eta 19:21:50
epoch [10/200] batch [10/25] time 12.932 (13.986) data 0.006 (0.233) loss 2.3041 (2.1896) acc 25.0000 (31.8750) lr 1.9921e-03 eta 18:30:43
epoch [10/200] batch [15/25] time 12.926 (13.648) data 0.006 (0.157) loss 2.5107 (2.3045) acc 28.1250 (28.1250) lr 1.9921e-03 eta 18:02:46
epoch [10/200] batch [20/25] time 12.921 (13.478) data 0.006 (0.120) loss 2.5234 (2.3085) acc 21.8750 (27.0312) lr 1.9921e-03 eta 17:48:06
epoch [10/200] batch [25/25] time 12.944 (13.391) data 0.006 (0.097) loss 2.4448 (2.3256) acc 18.7500 (25.7500) lr 1.9900e-03 eta 17:40:07
epoch [11/200] batch [5/25] time 13.358 (14.676) data 0.006 (0.503) loss 2.2586 (2.2380) acc 28.1250 (30.0000) lr 1.9900e-03 eta 19:20:39
epoch [11/200] batch [10/25] time 12.925 (13.972) data 0.004 (0.255) loss 2.0578 (2.2216) acc 31.2500 (27.8125) lr 1.9900e-03 eta 18:23:47
epoch [11/200] batch [15/25] time 12.933 (13.631) data 0.006 (0.172) loss 2.0491 (2.2179) acc 37.5000 (27.9167) lr 1.9900e-03 eta 17:55:42
epoch [11/200] batch [20/25] time 12.958 (13.462) data 0.005 (0.131) loss 2.6104 (2.2630) acc 15.6250 (26.8750) lr 1.9900e-03 eta 17:41:15
epoch [11/200] batch [25/25] time 12.940 (13.365) data 0.005 (0.106) loss 2.2308 (2.2706) acc 21.8750 (27.1250) lr 1.9877e-03 eta 17:32:30
epoch [12/200] batch [5/25] time 13.330 (14.484) data 0.006 (0.492) loss 2.1840 (2.3215) acc 18.7500 (28.1250) lr 1.9877e-03 eta 18:59:23
epoch [12/200] batch [10/25] time 12.956 (13.895) data 0.004 (0.251) loss 2.2614 (2.2944) acc 31.2500 (27.8125) lr 1.9877e-03 eta 18:11:55
epoch [12/200] batch [15/25] time 12.929 (13.578) data 0.006 (0.169) loss 2.4734 (2.3324) acc 21.8750 (27.9167) lr 1.9877e-03 eta 17:45:51
epoch [12/200] batch [20/25] time 12.979 (13.459) data 0.006 (0.128) loss 2.2003 (2.2972) acc 21.8750 (27.9688) lr 1.9877e-03 eta 17:35:25
epoch [12/200] batch [25/25] time 12.972 (13.399) data 0.006 (0.104) loss 2.3664 (2.2969) acc 25.0000 (28.2500) lr 1.9851e-03 eta 17:29:34
epoch [13/200] batch [5/25] time 13.376 (14.486) data 0.004 (0.485) loss 2.2013 (2.1778) acc 25.0000 (26.8750) lr 1.9851e-03 eta 18:53:31
epoch [13/200] batch [10/25] time 12.944 (13.892) data 0.009 (0.246) loss 2.1736 (2.2754) acc 28.1250 (24.0625) lr 1.9851e-03 eta 18:05:52
epoch [13/200] batch [15/25] time 12.961 (13.578) data 0.004 (0.166) loss 2.1811 (2.2669) acc 28.1250 (25.2083) lr 1.9851e-03 eta 17:40:14
epoch [13/200] batch [20/25] time 12.954 (13.428) data 0.006 (0.126) loss 2.2594 (2.2742) acc 21.8750 (25.0000) lr 1.9851e-03 eta 17:27:22
epoch [13/200] batch [25/25] time 13.022 (13.343) data 0.006 (0.102) loss 1.9239 (2.2338) acc 37.5000 (26.5000) lr 1.9823e-03 eta 17:19:38
epoch [14/200] batch [5/25] time 13.388 (14.692) data 0.006 (0.775) loss 2.2010 (2.2382) acc 31.2500 (26.8750) lr 1.9823e-03 eta 19:03:29
epoch [14/200] batch [10/25] time 12.958 (14.050) data 0.005 (0.398) loss 2.4009 (2.2755) acc 34.3750 (27.1875) lr 1.9823e-03 eta 18:12:23
epoch [14/200] batch [15/25] time 12.910 (13.693) data 0.005 (0.267) loss 2.1788 (2.2281) acc 31.2500 (28.3333) lr 1.9823e-03 eta 17:43:31
epoch [14/200] batch [20/25] time 12.932 (13.518) data 0.006 (0.202) loss 2.1147 (2.2111) acc 28.1250 (29.3750) lr 1.9823e-03 eta 17:28:48
epoch [14/200] batch [25/25] time 13.094 (13.444) data 0.006 (0.162) loss 2.6672 (2.2507) acc 18.7500 (28.6250) lr 1.9792e-03 eta 17:21:52
epoch [15/200] batch [5/25] time 13.344 (14.448) data 0.005 (0.461) loss 2.1115 (2.3316) acc 37.5000 (28.1250) lr 1.9792e-03 eta 18:38:32
epoch [15/200] batch [10/25] time 13.011 (13.870) data 0.009 (0.234) loss 2.1258 (2.2922) acc 34.3750 (29.3750) lr 1.9792e-03 eta 17:52:35
epoch [15/200] batch [15/25] time 12.955 (13.593) data 0.011 (0.158) loss 2.4880 (2.2711) acc 9.3750 (29.5833) lr 1.9792e-03 eta 17:30:03
epoch [15/200] batch [20/25] time 12.947 (13.478) data 0.005 (0.120) loss 2.5864 (2.2981) acc 15.6250 (27.9688) lr 1.9792e-03 eta 17:20:03
epoch [15/200] batch [25/25] time 12.904 (13.371) data 0.006 (0.097) loss 2.2076 (2.2741) acc 34.3750 (28.1250) lr 1.9759e-03 eta 17:10:41
epoch [16/200] batch [5/25] time 13.365 (14.503) data 0.006 (0.579) loss 2.0786 (2.1793) acc 34.3750 (32.5000) lr 1.9759e-03 eta 18:36:42
epoch [16/200] batch [10/25] time 12.922 (13.901) data 0.004 (0.293) loss 2.2775 (2.2033) acc 31.2500 (29.6875) lr 1.9759e-03 eta 17:49:13
epoch [16/200] batch [15/25] time 12.934 (13.587) data 0.004 (0.197) loss 2.0638 (2.2129) acc 34.3750 (29.7917) lr 1.9759e-03 eta 17:23:55
epoch [16/200] batch [20/25] time 12.926 (13.429) data 0.006 (0.149) loss 2.1908 (2.2318) acc 37.5000 (29.3750) lr 1.9759e-03 eta 17:10:41
epoch [16/200] batch [25/25] time 12.912 (13.341) data 0.006 (0.121) loss 2.7550 (2.2326) acc 9.3750 (29.3750) lr 1.9724e-03 eta 17:02:46
epoch [17/200] batch [5/25] time 13.486 (14.539) data 0.006 (0.584) loss 2.0202 (2.0292) acc 34.3750 (37.5000) lr 1.9724e-03 eta 18:33:26
epoch [17/200] batch [10/25] time 12.940 (13.921) data 0.004 (0.295) loss 2.4566 (2.1818) acc 21.8750 (32.8125) lr 1.9724e-03 eta 17:44:56
epoch [17/200] batch [15/25] time 12.953 (13.614) data 0.004 (0.198) loss 2.4633 (2.2284) acc 37.5000 (30.0000) lr 1.9724e-03 eta 17:20:20
